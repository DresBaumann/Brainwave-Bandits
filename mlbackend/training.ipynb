{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn transformers\n",
    "!pip install torch\n",
    "!pip install pandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create set of test wines (10)\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_csv('data/XWines_Full_100K_wines.csv').sample(10).to_csv('data/wine_small.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed with status code: 500\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the API endpoint\n",
    "url = \"http://172.28.0.186:5002/predictwine\"\n",
    "\n",
    "# Define the payload with the food description\n",
    "data = {\n",
    "    \"mainIngredient\": {\"Name\":\"Meat\"}\n",
    "}\n",
    "\n",
    "# Send a POST request to the API\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse and print the results\n",
    "    results = response.json()\n",
    "    for result in results:\n",
    "        print(f\"Wine ID: {result['wine_id']}, Is Pairing: {result['is_pairing']}\")\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"label_encoders.pkl\", \"rb\") as f:\n",
    "    label_encoders = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WineID</th>\n",
       "      <th>WineName</th>\n",
       "      <th>Type</th>\n",
       "      <th>Elaborate</th>\n",
       "      <th>Grapes</th>\n",
       "      <th>Harmonize</th>\n",
       "      <th>ABV</th>\n",
       "      <th>Body</th>\n",
       "      <th>Acidity</th>\n",
       "      <th>Code</th>\n",
       "      <th>Country</th>\n",
       "      <th>RegionID</th>\n",
       "      <th>RegionName</th>\n",
       "      <th>WineryID</th>\n",
       "      <th>WineryName</th>\n",
       "      <th>Website</th>\n",
       "      <th>Vintages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>Espumante Moscatel</td>\n",
       "      <td>Sparkling</td>\n",
       "      <td>Varietal/100%</td>\n",
       "      <td>['Muscat/Moscato']</td>\n",
       "      <td>['Pork', 'Rich Fish', 'Shellfish']</td>\n",
       "      <td>7.5</td>\n",
       "      <td>Medium-bodied</td>\n",
       "      <td>High</td>\n",
       "      <td>BR</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>1001</td>\n",
       "      <td>Serra Gaúcha</td>\n",
       "      <td>10001</td>\n",
       "      <td>Casa Perini</td>\n",
       "      <td>http://www.vinicolaperini.com.br</td>\n",
       "      <td>[2020, 2019, 2018, 2017, 2016, 2015, 2014, 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100002</td>\n",
       "      <td>Ancellotta</td>\n",
       "      <td>Red</td>\n",
       "      <td>Varietal/100%</td>\n",
       "      <td>['Ancellotta']</td>\n",
       "      <td>['Beef', 'Barbecue', 'Codfish', 'Pasta', 'Pizz...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Medium-bodied</td>\n",
       "      <td>Medium</td>\n",
       "      <td>BR</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>1001</td>\n",
       "      <td>Serra Gaúcha</td>\n",
       "      <td>10001</td>\n",
       "      <td>Casa Perini</td>\n",
       "      <td>http://www.vinicolaperini.com.br</td>\n",
       "      <td>[2016, 2015, 2014, 2013, 2012, 2011, 2010, 200...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   WineID            WineName       Type      Elaborate              Grapes  \\\n",
       "0  100001  Espumante Moscatel  Sparkling  Varietal/100%  ['Muscat/Moscato']   \n",
       "1  100002          Ancellotta        Red  Varietal/100%      ['Ancellotta']   \n",
       "\n",
       "                                           Harmonize   ABV           Body  \\\n",
       "0                 ['Pork', 'Rich Fish', 'Shellfish']   7.5  Medium-bodied   \n",
       "1  ['Beef', 'Barbecue', 'Codfish', 'Pasta', 'Pizz...  12.0  Medium-bodied   \n",
       "\n",
       "  Acidity Code Country  RegionID    RegionName  WineryID   WineryName  \\\n",
       "0    High   BR  Brazil      1001  Serra Gaúcha     10001  Casa Perini   \n",
       "1  Medium   BR  Brazil      1001  Serra Gaúcha     10001  Casa Perini   \n",
       "\n",
       "                            Website  \\\n",
       "0  http://www.vinicolaperini.com.br   \n",
       "1  http://www.vinicolaperini.com.br   \n",
       "\n",
       "                                            Vintages  \n",
       "0  [2020, 2019, 2018, 2017, 2016, 2015, 2014, 201...  \n",
       "1  [2016, 2015, 2014, 2013, 2012, 2011, 2010, 200...  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/XWines_Full_100K_wines.csv')\n",
    "\n",
    "# take the first 50k rows\n",
    "df = df.head(50000)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "df['Harmonize'] = df['Harmonize'].apply(ast.literal_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode('Harmonize', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"WineID\",\"Type\",\"Elaborate\",'Body', 'Acidity',\"ABV\",\"Harmonize\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pairing\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "categorical_columns = ['Type', 'Elaborate', 'Body', 'Acidity']\n",
    "label_encoders = {}\n",
    "\n",
    "# Fit and transform each column with its own LabelEncoder\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le  # Store the LabelEncoder for each column\n",
    "\n",
    "# Save the dictionary of LabelEncoders\n",
    "with open('label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# Generate random non-pairings (assume bad pairings)\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def generate_non_pairings(df):\n",
    "    neg_pairings = []\n",
    "    num_samples = len(df)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        food = df.iloc[i]['Harmonize']  # The current food description\n",
    "        original_wine = df.iloc[i]['WineID']  # The original wine ID paired with the food\n",
    "        \n",
    "        # Randomly select a different wine\n",
    "        while True:\n",
    "            random_idx = random.randint(0, num_samples - 1)\n",
    "            random_wine = df.iloc[random_idx]['WineID']\n",
    "            \n",
    "            # Ensure the random wine is different from the original wine\n",
    "            if random_wine != original_wine:\n",
    "                break\n",
    "        \n",
    "        # Extract the features for the randomly selected wine\n",
    "        wine_features = df.iloc[random_idx][['Type', 'Elaborate', 'Body', 'Acidity', 'ABV']].values\n",
    "        \n",
    "        # Add the non-pairing to the list\n",
    "        neg_pairings.append([random_wine, *wine_features, food, 0])  # Assuming '0' indicates a bad pairing\n",
    "\n",
    "    # Create a DataFrame with the same columns as the original\n",
    "    return pd.DataFrame(neg_pairings, columns=['WineID', 'Type', 'Elaborate', 'Body', 'Acidity', 'ABV', 'Harmonize', 'pairing'])\n",
    "\n",
    "# Create non-pairings\n",
    "non_pairings_df = generate_non_pairings(df)\n",
    "\n",
    "# Combine original good pairings with generated non-pairings\n",
    "combined_df = pd.concat([df, non_pairings_df])\n",
    "\n",
    "# Reset index\n",
    "combined_df = combined_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the tokenizer and model directly from transformers\n",
    "# Load the tokenizer and model directly from transformers\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n",
    "model = AutoModel.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "def get_text_embeddings(text, tokenizer, model):\n",
    "    # Tokenize input text\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=50)\n",
    "    \n",
    "    # Compute token embeddings without computing gradients\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    # Perform pooling: In this case, we'll use CLS token pooling (first token)\n",
    "    sentence_embeddings = model_output[0][:, 0]\n",
    "    \n",
    "    # Normalize the embeddings\n",
    "    #sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    return sentence_embeddings.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming combined_df is your DataFrame with a 'Food Description' column\n",
    "text_embeddings = np.vstack(\n",
    "    [get_text_embeddings(text, tokenizer, model) for text in tqdm(combined_df['Harmonize'], desc=\"Generating Embeddings\",total=len(combined_df))]\n",
    ")\n",
    "\n",
    "# Add embeddings to DataFrame\n",
    "text_embedding_df = pd.DataFrame(text_embeddings, columns=[f'emb_{i}' for i in range(text_embeddings.shape[1])])\n",
    "combined_df = pd.concat([combined_df, text_embedding_df], axis=1)\n",
    "\n",
    "# Display the combined DataFrame with embeddings\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features: Wine features + text embeddings\n",
    "\n",
    "# drop WINEID\n",
    "combined_df = combined_df.drop(['WineID','Harmonize'], axis=1)\n",
    "#combined_df = combined_df.drop(['WineID'], axis=1)\n",
    "# X is all but not pairing\n",
    "X = combined_df.drop(['pairing'], axis=1)\n",
    "\n",
    "# Target: Pairing Score (0 or 1)\n",
    "y = combined_df['pairing']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the random forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print feature importance\n",
    "feature_importance = pd.DataFrame(rf_model.feature_importances_, index=X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the tokenizer and model directly from transformers\n",
    "# Load the tokenizer and model directly from transformers\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n",
    "model = AutoModel.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "def get_text_embeddings(text, tokenizer, model):\n",
    "    # Tokenize input text\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=50)\n",
    "    \n",
    "    # Compute token embeddings without computing gradients\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    # Perform pooling: In this case, we'll use CLS token pooling (first token)\n",
    "    sentence_embeddings = model_output[0][:, 0]\n",
    "    \n",
    "    # Normalize the embeddings\n",
    "    #sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    return sentence_embeddings.numpy()\n",
    "\n",
    "# Assume we have a DataFrame `df` with 'Food Description' and wine features\n",
    "# Wine features might include columns like 'Acidity', 'Body', 'Tannins', 'ABV', etc.\n",
    "# Normalize the wine features\n",
    "\"\"\" scaler = StandardScaler()\n",
    "wine_features = scaler.fit_transform(df[['Type', 'Body', 'Tannins']]) \"\"\"\n",
    "\n",
    "wine_features = combined_df[['Type', 'Elaborate','Body', 'Acidity', 'ABV']].values\n",
    "\n",
    "# Generate text embeddings for the food descriptions\n",
    "text_embeddings = []\n",
    "for text in tqdm(combined_df['Harmonize'],total=len(combined_df)):\n",
    "    emb = get_text_embeddings(text, tokenizer, model)\n",
    "    text_embeddings.append(emb)\n",
    "\n",
    "text_embeddings = np.vstack(text_embeddings)\n",
    "\n",
    "# Combine text embeddings and wine features\n",
    "combined_features = np.hstack([text_embeddings, wine_features])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, combined_df['pairing'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save text_embeddings to disk\n",
    "np.save('data/text_embeddings.npy', text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read text_embeddings from disk\n",
    "text_embeddings = np.load('data/text_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_features = combined_df[['Type', 'Elaborate','Body', 'Acidity', 'ABV']].values\n",
    "\n",
    "# Combine text embeddings and wine features\n",
    "combined_features = np.hstack([text_embeddings, wine_features])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, combined_df['pairing'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/600], Loss: 0.6934\n",
      "Epoch [2/600], Loss: 0.6938\n",
      "Epoch [3/600], Loss: 0.6925\n",
      "Epoch [4/600], Loss: 0.6921\n",
      "Epoch [5/600], Loss: 0.6919\n",
      "Epoch [6/600], Loss: 0.6913\n",
      "Epoch [7/600], Loss: 0.6908\n",
      "Epoch [8/600], Loss: 0.6904\n",
      "Epoch [9/600], Loss: 0.6899\n",
      "Epoch [10/600], Loss: 0.6894\n",
      "Epoch [11/600], Loss: 0.6889\n",
      "Epoch [12/600], Loss: 0.6883\n",
      "Epoch [13/600], Loss: 0.6876\n",
      "Epoch [14/600], Loss: 0.6870\n",
      "Epoch [15/600], Loss: 0.6862\n",
      "Epoch [16/600], Loss: 0.6854\n",
      "Epoch [17/600], Loss: 0.6845\n",
      "Epoch [18/600], Loss: 0.6835\n",
      "Epoch [19/600], Loss: 0.6822\n",
      "Epoch [20/600], Loss: 0.6810\n",
      "Epoch [21/600], Loss: 0.6799\n",
      "Epoch [22/600], Loss: 0.6786\n",
      "Epoch [23/600], Loss: 0.6772\n",
      "Epoch [24/600], Loss: 0.6756\n",
      "Epoch [25/600], Loss: 0.6740\n",
      "Epoch [26/600], Loss: 0.6723\n",
      "Epoch [27/600], Loss: 0.6704\n",
      "Epoch [28/600], Loss: 0.6685\n",
      "Epoch [29/600], Loss: 0.6665\n",
      "Epoch [30/600], Loss: 0.6645\n",
      "Epoch [31/600], Loss: 0.6623\n",
      "Epoch [32/600], Loss: 0.6600\n",
      "Epoch [33/600], Loss: 0.6576\n",
      "Epoch [34/600], Loss: 0.6550\n",
      "Epoch [35/600], Loss: 0.6524\n",
      "Epoch [36/600], Loss: 0.6498\n",
      "Epoch [37/600], Loss: 0.6471\n",
      "Epoch [38/600], Loss: 0.6444\n",
      "Epoch [39/600], Loss: 0.6416\n",
      "Epoch [40/600], Loss: 0.6388\n",
      "Epoch [41/600], Loss: 0.6361\n",
      "Epoch [42/600], Loss: 0.6336\n",
      "Epoch [43/600], Loss: 0.6313\n",
      "Epoch [44/600], Loss: 0.6281\n",
      "Epoch [45/600], Loss: 0.6251\n",
      "Epoch [46/600], Loss: 0.6230\n",
      "Epoch [47/600], Loss: 0.6204\n",
      "Epoch [48/600], Loss: 0.6174\n",
      "Epoch [49/600], Loss: 0.6152\n",
      "Epoch [50/600], Loss: 0.6134\n",
      "Epoch [51/600], Loss: 0.6111\n",
      "Epoch [52/600], Loss: 0.6086\n",
      "Epoch [53/600], Loss: 0.6066\n",
      "Epoch [54/600], Loss: 0.6050\n",
      "Epoch [55/600], Loss: 0.6032\n",
      "Epoch [56/600], Loss: 0.6011\n",
      "Epoch [57/600], Loss: 0.5991\n",
      "Epoch [58/600], Loss: 0.5976\n",
      "Epoch [59/600], Loss: 0.5964\n",
      "Epoch [60/600], Loss: 0.5953\n",
      "Epoch [61/600], Loss: 0.5944\n",
      "Epoch [62/600], Loss: 0.5924\n",
      "Epoch [63/600], Loss: 0.5907\n",
      "Epoch [64/600], Loss: 0.5900\n",
      "Epoch [65/600], Loss: 0.5891\n",
      "Epoch [66/600], Loss: 0.5884\n",
      "Epoch [67/600], Loss: 0.5872\n",
      "Epoch [68/600], Loss: 0.5859\n",
      "Epoch [69/600], Loss: 0.5851\n",
      "Epoch [70/600], Loss: 0.5844\n",
      "Epoch [71/600], Loss: 0.5841\n",
      "Epoch [72/600], Loss: 0.5835\n",
      "Epoch [73/600], Loss: 0.5829\n",
      "Epoch [74/600], Loss: 0.5819\n",
      "Epoch [75/600], Loss: 0.5809\n",
      "Epoch [76/600], Loss: 0.5803\n",
      "Epoch [77/600], Loss: 0.5799\n",
      "Epoch [78/600], Loss: 0.5797\n",
      "Epoch [79/600], Loss: 0.5794\n",
      "Epoch [80/600], Loss: 0.5790\n",
      "Epoch [81/600], Loss: 0.5782\n",
      "Epoch [82/600], Loss: 0.5774\n",
      "Epoch [83/600], Loss: 0.5768\n",
      "Epoch [84/600], Loss: 0.5764\n",
      "Epoch [85/600], Loss: 0.5762\n",
      "Epoch [86/600], Loss: 0.5761\n",
      "Epoch [87/600], Loss: 0.5761\n",
      "Epoch [88/600], Loss: 0.5756\n",
      "Epoch [89/600], Loss: 0.5749\n",
      "Epoch [90/600], Loss: 0.5737\n",
      "Epoch [91/600], Loss: 0.5731\n",
      "Epoch [92/600], Loss: 0.5729\n",
      "Epoch [93/600], Loss: 0.5730\n",
      "Epoch [94/600], Loss: 0.5727\n",
      "Epoch [95/600], Loss: 0.5719\n",
      "Epoch [96/600], Loss: 0.5711\n",
      "Epoch [97/600], Loss: 0.5705\n",
      "Epoch [98/600], Loss: 0.5702\n",
      "Epoch [99/600], Loss: 0.5701\n",
      "Epoch [100/600], Loss: 0.5700\n",
      "Epoch [101/600], Loss: 0.5697\n",
      "Epoch [102/600], Loss: 0.5691\n",
      "Epoch [103/600], Loss: 0.5684\n",
      "Epoch [104/600], Loss: 0.5677\n",
      "Epoch [105/600], Loss: 0.5673\n",
      "Epoch [106/600], Loss: 0.5671\n",
      "Epoch [107/600], Loss: 0.5670\n",
      "Epoch [108/600], Loss: 0.5670\n",
      "Epoch [109/600], Loss: 0.5670\n",
      "Epoch [110/600], Loss: 0.5670\n",
      "Epoch [111/600], Loss: 0.5663\n",
      "Epoch [112/600], Loss: 0.5655\n",
      "Epoch [113/600], Loss: 0.5645\n",
      "Epoch [114/600], Loss: 0.5641\n",
      "Epoch [115/600], Loss: 0.5641\n",
      "Epoch [116/600], Loss: 0.5643\n",
      "Epoch [117/600], Loss: 0.5644\n",
      "Epoch [118/600], Loss: 0.5640\n",
      "Epoch [119/600], Loss: 0.5632\n",
      "Epoch [120/600], Loss: 0.5623\n",
      "Epoch [121/600], Loss: 0.5618\n",
      "Epoch [122/600], Loss: 0.5619\n",
      "Epoch [123/600], Loss: 0.5620\n",
      "Epoch [124/600], Loss: 0.5620\n",
      "Epoch [125/600], Loss: 0.5614\n",
      "Epoch [126/600], Loss: 0.5607\n",
      "Epoch [127/600], Loss: 0.5599\n",
      "Epoch [128/600], Loss: 0.5595\n",
      "Epoch [129/600], Loss: 0.5594\n",
      "Epoch [130/600], Loss: 0.5595\n",
      "Epoch [131/600], Loss: 0.5596\n",
      "Epoch [132/600], Loss: 0.5595\n",
      "Epoch [133/600], Loss: 0.5593\n",
      "Epoch [134/600], Loss: 0.5585\n",
      "Epoch [135/600], Loss: 0.5576\n",
      "Epoch [136/600], Loss: 0.5571\n",
      "Epoch [137/600], Loss: 0.5570\n",
      "Epoch [138/600], Loss: 0.5571\n",
      "Epoch [139/600], Loss: 0.5572\n",
      "Epoch [140/600], Loss: 0.5574\n",
      "Epoch [141/600], Loss: 0.5571\n",
      "Epoch [142/600], Loss: 0.5565\n",
      "Epoch [143/600], Loss: 0.5556\n",
      "Epoch [144/600], Loss: 0.5548\n",
      "Epoch [145/600], Loss: 0.5545\n",
      "Epoch [146/600], Loss: 0.5546\n",
      "Epoch [147/600], Loss: 0.5549\n",
      "Epoch [148/600], Loss: 0.5552\n",
      "Epoch [149/600], Loss: 0.5554\n",
      "Epoch [150/600], Loss: 0.5548\n",
      "Epoch [151/600], Loss: 0.5539\n",
      "Epoch [152/600], Loss: 0.5528\n",
      "Epoch [153/600], Loss: 0.5523\n",
      "Epoch [154/600], Loss: 0.5525\n",
      "Epoch [155/600], Loss: 0.5529\n",
      "Epoch [156/600], Loss: 0.5532\n",
      "Epoch [157/600], Loss: 0.5528\n",
      "Epoch [158/600], Loss: 0.5520\n",
      "Epoch [159/600], Loss: 0.5511\n",
      "Epoch [160/600], Loss: 0.5505\n",
      "Epoch [161/600], Loss: 0.5504\n",
      "Epoch [162/600], Loss: 0.5507\n",
      "Epoch [163/600], Loss: 0.5511\n",
      "Epoch [164/600], Loss: 0.5511\n",
      "Epoch [165/600], Loss: 0.5509\n",
      "Epoch [166/600], Loss: 0.5501\n",
      "Epoch [167/600], Loss: 0.5492\n",
      "Epoch [168/600], Loss: 0.5486\n",
      "Epoch [169/600], Loss: 0.5486\n",
      "Epoch [170/600], Loss: 0.5489\n",
      "Epoch [171/600], Loss: 0.5492\n",
      "Epoch [172/600], Loss: 0.5495\n",
      "Epoch [173/600], Loss: 0.5492\n",
      "Epoch [174/600], Loss: 0.5484\n",
      "Epoch [175/600], Loss: 0.5474\n",
      "Epoch [176/600], Loss: 0.5468\n",
      "Epoch [177/600], Loss: 0.5468\n",
      "Epoch [178/600], Loss: 0.5471\n",
      "Epoch [179/600], Loss: 0.5474\n",
      "Epoch [180/600], Loss: 0.5475\n",
      "Epoch [181/600], Loss: 0.5474\n",
      "Epoch [182/600], Loss: 0.5466\n",
      "Epoch [183/600], Loss: 0.5458\n",
      "Epoch [184/600], Loss: 0.5451\n",
      "Epoch [185/600], Loss: 0.5449\n",
      "Epoch [186/600], Loss: 0.5451\n",
      "Epoch [187/600], Loss: 0.5454\n",
      "Epoch [188/600], Loss: 0.5458\n",
      "Epoch [189/600], Loss: 0.5458\n",
      "Epoch [190/600], Loss: 0.5455\n",
      "Epoch [191/600], Loss: 0.5447\n",
      "Epoch [192/600], Loss: 0.5438\n",
      "Epoch [193/600], Loss: 0.5433\n",
      "Epoch [194/600], Loss: 0.5432\n",
      "Epoch [195/600], Loss: 0.5434\n",
      "Epoch [196/600], Loss: 0.5438\n",
      "Epoch [197/600], Loss: 0.5442\n",
      "Epoch [198/600], Loss: 0.5442\n",
      "Epoch [199/600], Loss: 0.5439\n",
      "Epoch [200/600], Loss: 0.5430\n",
      "Epoch [201/600], Loss: 0.5422\n",
      "Epoch [202/600], Loss: 0.5417\n",
      "Epoch [203/600], Loss: 0.5417\n",
      "Epoch [204/600], Loss: 0.5420\n",
      "Epoch [205/600], Loss: 0.5423\n",
      "Epoch [206/600], Loss: 0.5426\n",
      "Epoch [207/600], Loss: 0.5425\n",
      "Epoch [208/600], Loss: 0.5422\n",
      "Epoch [209/600], Loss: 0.5413\n",
      "Epoch [210/600], Loss: 0.5406\n",
      "Epoch [211/600], Loss: 0.5402\n",
      "Epoch [212/600], Loss: 0.5402\n",
      "Epoch [213/600], Loss: 0.5404\n",
      "Epoch [214/600], Loss: 0.5407\n",
      "Epoch [215/600], Loss: 0.5412\n",
      "Epoch [216/600], Loss: 0.5412\n",
      "Epoch [217/600], Loss: 0.5412\n",
      "Epoch [218/600], Loss: 0.5404\n",
      "Epoch [219/600], Loss: 0.5397\n",
      "Epoch [220/600], Loss: 0.5390\n",
      "Epoch [221/600], Loss: 0.5387\n",
      "Epoch [222/600], Loss: 0.5388\n",
      "Epoch [223/600], Loss: 0.5391\n",
      "Epoch [224/600], Loss: 0.5396\n",
      "Epoch [225/600], Loss: 0.5399\n",
      "Epoch [226/600], Loss: 0.5402\n",
      "Epoch [227/600], Loss: 0.5398\n",
      "Epoch [228/600], Loss: 0.5392\n",
      "Epoch [229/600], Loss: 0.5382\n",
      "Epoch [230/600], Loss: 0.5375\n",
      "Epoch [231/600], Loss: 0.5374\n",
      "Epoch [232/600], Loss: 0.5377\n",
      "Epoch [233/600], Loss: 0.5382\n",
      "Epoch [234/600], Loss: 0.5385\n",
      "Epoch [235/600], Loss: 0.5386\n",
      "Epoch [236/600], Loss: 0.5382\n",
      "Epoch [237/600], Loss: 0.5376\n",
      "Epoch [238/600], Loss: 0.5368\n",
      "Epoch [239/600], Loss: 0.5363\n",
      "Epoch [240/600], Loss: 0.5362\n",
      "Epoch [241/600], Loss: 0.5364\n",
      "Epoch [242/600], Loss: 0.5367\n",
      "Epoch [243/600], Loss: 0.5370\n",
      "Epoch [244/600], Loss: 0.5374\n",
      "Epoch [245/600], Loss: 0.5373\n",
      "Epoch [246/600], Loss: 0.5371\n",
      "Epoch [247/600], Loss: 0.5363\n",
      "Epoch [248/600], Loss: 0.5356\n",
      "Epoch [249/600], Loss: 0.5351\n",
      "Epoch [250/600], Loss: 0.5350\n",
      "Epoch [251/600], Loss: 0.5352\n",
      "Epoch [252/600], Loss: 0.5355\n",
      "Epoch [253/600], Loss: 0.5359\n",
      "Epoch [254/600], Loss: 0.5362\n",
      "Epoch [255/600], Loss: 0.5364\n",
      "Epoch [256/600], Loss: 0.5360\n",
      "Epoch [257/600], Loss: 0.5355\n",
      "Epoch [258/600], Loss: 0.5346\n",
      "Epoch [259/600], Loss: 0.5340\n",
      "Epoch [260/600], Loss: 0.5338\n",
      "Epoch [261/600], Loss: 0.5340\n",
      "Epoch [262/600], Loss: 0.5343\n",
      "Epoch [263/600], Loss: 0.5347\n",
      "Epoch [264/600], Loss: 0.5352\n",
      "Epoch [265/600], Loss: 0.5352\n",
      "Epoch [266/600], Loss: 0.5353\n",
      "Epoch [267/600], Loss: 0.5346\n",
      "Epoch [268/600], Loss: 0.5339\n",
      "Epoch [269/600], Loss: 0.5331\n",
      "Epoch [270/600], Loss: 0.5327\n",
      "Epoch [271/600], Loss: 0.5327\n",
      "Epoch [272/600], Loss: 0.5329\n",
      "Epoch [273/600], Loss: 0.5334\n",
      "Epoch [274/600], Loss: 0.5338\n",
      "Epoch [275/600], Loss: 0.5343\n",
      "Epoch [276/600], Loss: 0.5344\n",
      "Epoch [277/600], Loss: 0.5344\n",
      "Epoch [278/600], Loss: 0.5336\n",
      "Epoch [279/600], Loss: 0.5326\n",
      "Epoch [280/600], Loss: 0.5318\n",
      "Epoch [281/600], Loss: 0.5316\n",
      "Epoch [282/600], Loss: 0.5319\n",
      "Epoch [283/600], Loss: 0.5324\n",
      "Epoch [284/600], Loss: 0.5329\n",
      "Epoch [285/600], Loss: 0.5330\n",
      "Epoch [286/600], Loss: 0.5329\n",
      "Epoch [287/600], Loss: 0.5323\n",
      "Epoch [288/600], Loss: 0.5315\n",
      "Epoch [289/600], Loss: 0.5309\n",
      "Epoch [290/600], Loss: 0.5306\n",
      "Epoch [291/600], Loss: 0.5307\n",
      "Epoch [292/600], Loss: 0.5310\n",
      "Epoch [293/600], Loss: 0.5315\n",
      "Epoch [294/600], Loss: 0.5319\n",
      "Epoch [295/600], Loss: 0.5324\n",
      "Epoch [296/600], Loss: 0.5323\n",
      "Epoch [297/600], Loss: 0.5321\n",
      "Epoch [298/600], Loss: 0.5311\n",
      "Epoch [299/600], Loss: 0.5303\n",
      "Epoch [300/600], Loss: 0.5298\n",
      "Epoch [301/600], Loss: 0.5297\n",
      "Epoch [302/600], Loss: 0.5301\n",
      "Epoch [303/600], Loss: 0.5306\n",
      "Epoch [304/600], Loss: 0.5312\n",
      "Epoch [305/600], Loss: 0.5314\n",
      "Epoch [306/600], Loss: 0.5316\n",
      "Epoch [307/600], Loss: 0.5309\n",
      "Epoch [308/600], Loss: 0.5302\n",
      "Epoch [309/600], Loss: 0.5293\n",
      "Epoch [310/600], Loss: 0.5289\n",
      "Epoch [311/600], Loss: 0.5289\n",
      "Epoch [312/600], Loss: 0.5292\n",
      "Epoch [313/600], Loss: 0.5297\n",
      "Epoch [314/600], Loss: 0.5300\n",
      "Epoch [315/600], Loss: 0.5304\n",
      "Epoch [316/600], Loss: 0.5303\n",
      "Epoch [317/600], Loss: 0.5301\n",
      "Epoch [318/600], Loss: 0.5293\n",
      "Epoch [319/600], Loss: 0.5287\n",
      "Epoch [320/600], Loss: 0.5281\n",
      "Epoch [321/600], Loss: 0.5280\n",
      "Epoch [322/600], Loss: 0.5281\n",
      "Epoch [323/600], Loss: 0.5284\n",
      "Epoch [324/600], Loss: 0.5289\n",
      "Epoch [325/600], Loss: 0.5292\n",
      "Epoch [326/600], Loss: 0.5297\n",
      "Epoch [327/600], Loss: 0.5297\n",
      "Epoch [328/600], Loss: 0.5295\n",
      "Epoch [329/600], Loss: 0.5287\n",
      "Epoch [330/600], Loss: 0.5280\n",
      "Epoch [331/600], Loss: 0.5273\n",
      "Epoch [332/600], Loss: 0.5271\n",
      "Epoch [333/600], Loss: 0.5273\n",
      "Epoch [334/600], Loss: 0.5277\n",
      "Epoch [335/600], Loss: 0.5282\n",
      "Epoch [336/600], Loss: 0.5286\n",
      "Epoch [337/600], Loss: 0.5290\n",
      "Epoch [338/600], Loss: 0.5289\n",
      "Epoch [339/600], Loss: 0.5287\n",
      "Epoch [340/600], Loss: 0.5278\n",
      "Epoch [341/600], Loss: 0.5269\n",
      "Epoch [342/600], Loss: 0.5264\n",
      "Epoch [343/600], Loss: 0.5264\n",
      "Epoch [344/600], Loss: 0.5267\n",
      "Epoch [345/600], Loss: 0.5271\n",
      "Epoch [346/600], Loss: 0.5278\n",
      "Epoch [347/600], Loss: 0.5282\n",
      "Epoch [348/600], Loss: 0.5287\n",
      "Epoch [349/600], Loss: 0.5284\n",
      "Epoch [350/600], Loss: 0.5277\n",
      "Epoch [351/600], Loss: 0.5266\n",
      "Epoch [352/600], Loss: 0.5258\n",
      "Epoch [353/600], Loss: 0.5257\n",
      "Epoch [354/600], Loss: 0.5260\n",
      "Epoch [355/600], Loss: 0.5266\n",
      "Epoch [356/600], Loss: 0.5271\n",
      "Epoch [357/600], Loss: 0.5276\n",
      "Epoch [358/600], Loss: 0.5274\n",
      "Epoch [359/600], Loss: 0.5271\n",
      "Epoch [360/600], Loss: 0.5262\n",
      "Epoch [361/600], Loss: 0.5254\n",
      "Epoch [362/600], Loss: 0.5251\n",
      "Epoch [363/600], Loss: 0.5251\n",
      "Epoch [364/600], Loss: 0.5254\n",
      "Epoch [365/600], Loss: 0.5258\n",
      "Epoch [366/600], Loss: 0.5263\n",
      "Epoch [367/600], Loss: 0.5266\n",
      "Epoch [368/600], Loss: 0.5269\n",
      "Epoch [369/600], Loss: 0.5266\n",
      "Epoch [370/600], Loss: 0.5261\n",
      "Epoch [371/600], Loss: 0.5253\n",
      "Epoch [372/600], Loss: 0.5246\n",
      "Epoch [373/600], Loss: 0.5243\n",
      "Epoch [374/600], Loss: 0.5244\n",
      "Epoch [375/600], Loss: 0.5247\n",
      "Epoch [376/600], Loss: 0.5251\n",
      "Epoch [377/600], Loss: 0.5257\n",
      "Epoch [378/600], Loss: 0.5262\n",
      "Epoch [379/600], Loss: 0.5268\n",
      "Epoch [380/600], Loss: 0.5268\n",
      "Epoch [381/600], Loss: 0.5265\n",
      "Epoch [382/600], Loss: 0.5253\n",
      "Epoch [383/600], Loss: 0.5242\n",
      "Epoch [384/600], Loss: 0.5236\n",
      "Epoch [385/600], Loss: 0.5237\n",
      "Epoch [386/600], Loss: 0.5243\n",
      "Epoch [387/600], Loss: 0.5250\n",
      "Epoch [388/600], Loss: 0.5258\n",
      "Epoch [389/600], Loss: 0.5258\n",
      "Epoch [390/600], Loss: 0.5256\n",
      "Epoch [391/600], Loss: 0.5246\n",
      "Epoch [392/600], Loss: 0.5236\n",
      "Epoch [393/600], Loss: 0.5231\n",
      "Epoch [394/600], Loss: 0.5230\n",
      "Epoch [395/600], Loss: 0.5235\n",
      "Epoch [396/600], Loss: 0.5240\n",
      "Epoch [397/600], Loss: 0.5247\n",
      "Epoch [398/600], Loss: 0.5249\n",
      "Epoch [399/600], Loss: 0.5252\n",
      "Epoch [400/600], Loss: 0.5247\n",
      "Epoch [401/600], Loss: 0.5242\n",
      "Epoch [402/600], Loss: 0.5233\n",
      "Epoch [403/600], Loss: 0.5226\n",
      "Epoch [404/600], Loss: 0.5224\n",
      "Epoch [405/600], Loss: 0.5224\n",
      "Epoch [406/600], Loss: 0.5228\n",
      "Epoch [407/600], Loss: 0.5233\n",
      "Epoch [408/600], Loss: 0.5239\n",
      "Epoch [409/600], Loss: 0.5243\n",
      "Epoch [410/600], Loss: 0.5248\n",
      "Epoch [411/600], Loss: 0.5245\n",
      "Epoch [412/600], Loss: 0.5240\n",
      "Epoch [413/600], Loss: 0.5231\n",
      "Epoch [414/600], Loss: 0.5222\n",
      "Epoch [415/600], Loss: 0.5218\n",
      "Epoch [416/600], Loss: 0.5218\n",
      "Epoch [417/600], Loss: 0.5223\n",
      "Epoch [418/600], Loss: 0.5228\n",
      "Epoch [419/600], Loss: 0.5233\n",
      "Epoch [420/600], Loss: 0.5235\n",
      "Epoch [421/600], Loss: 0.5237\n",
      "Epoch [422/600], Loss: 0.5233\n",
      "Epoch [423/600], Loss: 0.5228\n",
      "Epoch [424/600], Loss: 0.5221\n",
      "Epoch [425/600], Loss: 0.5215\n",
      "Epoch [426/600], Loss: 0.5212\n",
      "Epoch [427/600], Loss: 0.5212\n",
      "Epoch [428/600], Loss: 0.5214\n",
      "Epoch [429/600], Loss: 0.5218\n",
      "Epoch [430/600], Loss: 0.5222\n",
      "Epoch [431/600], Loss: 0.5226\n",
      "Epoch [432/600], Loss: 0.5232\n",
      "Epoch [433/600], Loss: 0.5234\n",
      "Epoch [434/600], Loss: 0.5237\n",
      "Epoch [435/600], Loss: 0.5231\n",
      "Epoch [436/600], Loss: 0.5224\n",
      "Epoch [437/600], Loss: 0.5215\n",
      "Epoch [438/600], Loss: 0.5208\n",
      "Epoch [439/600], Loss: 0.5206\n",
      "Epoch [440/600], Loss: 0.5208\n",
      "Epoch [441/600], Loss: 0.5212\n",
      "Epoch [442/600], Loss: 0.5217\n",
      "Epoch [443/600], Loss: 0.5223\n",
      "Epoch [444/600], Loss: 0.5226\n",
      "Epoch [445/600], Loss: 0.5230\n",
      "Epoch [446/600], Loss: 0.5226\n",
      "Epoch [447/600], Loss: 0.5220\n",
      "Epoch [448/600], Loss: 0.5211\n",
      "Epoch [449/600], Loss: 0.5204\n",
      "Epoch [450/600], Loss: 0.5201\n",
      "Epoch [451/600], Loss: 0.5202\n",
      "Epoch [452/600], Loss: 0.5206\n",
      "Epoch [453/600], Loss: 0.5211\n",
      "Epoch [454/600], Loss: 0.5217\n",
      "Epoch [455/600], Loss: 0.5220\n",
      "Epoch [456/600], Loss: 0.5223\n",
      "Epoch [457/600], Loss: 0.5220\n",
      "Epoch [458/600], Loss: 0.5215\n",
      "Epoch [459/600], Loss: 0.5207\n",
      "Epoch [460/600], Loss: 0.5200\n",
      "Epoch [461/600], Loss: 0.5197\n",
      "Epoch [462/600], Loss: 0.5196\n",
      "Epoch [463/600], Loss: 0.5199\n",
      "Epoch [464/600], Loss: 0.5202\n",
      "Epoch [465/600], Loss: 0.5207\n",
      "Epoch [466/600], Loss: 0.5211\n",
      "Epoch [467/600], Loss: 0.5218\n",
      "Epoch [468/600], Loss: 0.5220\n",
      "Epoch [469/600], Loss: 0.5222\n",
      "Epoch [470/600], Loss: 0.5217\n",
      "Epoch [471/600], Loss: 0.5209\n",
      "Epoch [472/600], Loss: 0.5200\n",
      "Epoch [473/600], Loss: 0.5193\n",
      "Epoch [474/600], Loss: 0.5192\n",
      "Epoch [475/600], Loss: 0.5194\n",
      "Epoch [476/600], Loss: 0.5199\n",
      "Epoch [477/600], Loss: 0.5203\n",
      "Epoch [478/600], Loss: 0.5209\n",
      "Epoch [479/600], Loss: 0.5211\n",
      "Epoch [480/600], Loss: 0.5213\n",
      "Epoch [481/600], Loss: 0.5209\n",
      "Epoch [482/600], Loss: 0.5203\n",
      "Epoch [483/600], Loss: 0.5196\n",
      "Epoch [484/600], Loss: 0.5190\n",
      "Epoch [485/600], Loss: 0.5187\n",
      "Epoch [486/600], Loss: 0.5187\n",
      "Epoch [487/600], Loss: 0.5189\n",
      "Epoch [488/600], Loss: 0.5192\n",
      "Epoch [489/600], Loss: 0.5196\n",
      "Epoch [490/600], Loss: 0.5200\n",
      "Epoch [491/600], Loss: 0.5206\n",
      "Epoch [492/600], Loss: 0.5209\n",
      "Epoch [493/600], Loss: 0.5213\n",
      "Epoch [494/600], Loss: 0.5211\n",
      "Epoch [495/600], Loss: 0.5208\n",
      "Epoch [496/600], Loss: 0.5199\n",
      "Epoch [497/600], Loss: 0.5190\n",
      "Epoch [498/600], Loss: 0.5184\n",
      "Epoch [499/600], Loss: 0.5182\n",
      "Epoch [500/600], Loss: 0.5184\n",
      "Epoch [501/600], Loss: 0.5189\n",
      "Epoch [502/600], Loss: 0.5195\n",
      "Epoch [503/600], Loss: 0.5200\n",
      "Epoch [504/600], Loss: 0.5207\n",
      "Epoch [505/600], Loss: 0.5207\n",
      "Epoch [506/600], Loss: 0.5205\n",
      "Epoch [507/600], Loss: 0.5197\n",
      "Epoch [508/600], Loss: 0.5187\n",
      "Epoch [509/600], Loss: 0.5180\n",
      "Epoch [510/600], Loss: 0.5177\n",
      "Epoch [511/600], Loss: 0.5180\n",
      "Epoch [512/600], Loss: 0.5184\n",
      "Epoch [513/600], Loss: 0.5190\n",
      "Epoch [514/600], Loss: 0.5194\n",
      "Epoch [515/600], Loss: 0.5199\n",
      "Epoch [516/600], Loss: 0.5197\n",
      "Epoch [517/600], Loss: 0.5194\n",
      "Epoch [518/600], Loss: 0.5187\n",
      "Epoch [519/600], Loss: 0.5180\n",
      "Epoch [520/600], Loss: 0.5175\n",
      "Epoch [521/600], Loss: 0.5173\n",
      "Epoch [522/600], Loss: 0.5174\n",
      "Epoch [523/600], Loss: 0.5176\n",
      "Epoch [524/600], Loss: 0.5180\n",
      "Epoch [525/600], Loss: 0.5184\n",
      "Epoch [526/600], Loss: 0.5191\n",
      "Epoch [527/600], Loss: 0.5195\n",
      "Epoch [528/600], Loss: 0.5200\n",
      "Epoch [529/600], Loss: 0.5199\n",
      "Epoch [530/600], Loss: 0.5195\n",
      "Epoch [531/600], Loss: 0.5185\n",
      "Epoch [532/600], Loss: 0.5176\n",
      "Epoch [533/600], Loss: 0.5170\n",
      "Epoch [534/600], Loss: 0.5169\n",
      "Epoch [535/600], Loss: 0.5173\n",
      "Epoch [536/600], Loss: 0.5178\n",
      "Epoch [537/600], Loss: 0.5186\n",
      "Epoch [538/600], Loss: 0.5189\n",
      "Epoch [539/600], Loss: 0.5193\n",
      "Epoch [540/600], Loss: 0.5188\n",
      "Epoch [541/600], Loss: 0.5183\n",
      "Epoch [542/600], Loss: 0.5174\n",
      "Epoch [543/600], Loss: 0.5167\n",
      "Epoch [544/600], Loss: 0.5165\n",
      "Epoch [545/600], Loss: 0.5166\n",
      "Epoch [546/600], Loss: 0.5170\n",
      "Epoch [547/600], Loss: 0.5175\n",
      "Epoch [548/600], Loss: 0.5180\n",
      "Epoch [549/600], Loss: 0.5181\n",
      "Epoch [550/600], Loss: 0.5184\n",
      "Epoch [551/600], Loss: 0.5180\n",
      "Epoch [552/600], Loss: 0.5176\n",
      "Epoch [553/600], Loss: 0.5170\n",
      "Epoch [554/600], Loss: 0.5164\n",
      "Epoch [555/600], Loss: 0.5162\n",
      "Epoch [556/600], Loss: 0.5161\n",
      "Epoch [557/600], Loss: 0.5163\n",
      "Epoch [558/600], Loss: 0.5165\n",
      "Epoch [559/600], Loss: 0.5170\n",
      "Epoch [560/600], Loss: 0.5174\n",
      "Epoch [561/600], Loss: 0.5179\n",
      "Epoch [562/600], Loss: 0.5182\n",
      "Epoch [563/600], Loss: 0.5186\n",
      "Epoch [564/600], Loss: 0.5182\n",
      "Epoch [565/600], Loss: 0.5178\n",
      "Epoch [566/600], Loss: 0.5170\n",
      "Epoch [567/600], Loss: 0.5163\n",
      "Epoch [568/600], Loss: 0.5158\n",
      "Epoch [569/600], Loss: 0.5157\n",
      "Epoch [570/600], Loss: 0.5159\n",
      "Epoch [571/600], Loss: 0.5162\n",
      "Epoch [572/600], Loss: 0.5167\n",
      "Epoch [573/600], Loss: 0.5172\n",
      "Epoch [574/600], Loss: 0.5178\n",
      "Epoch [575/600], Loss: 0.5181\n",
      "Epoch [576/600], Loss: 0.5183\n",
      "Epoch [577/600], Loss: 0.5178\n",
      "Epoch [578/600], Loss: 0.5172\n",
      "Epoch [579/600], Loss: 0.5163\n",
      "Epoch [580/600], Loss: 0.5156\n",
      "Epoch [581/600], Loss: 0.5153\n",
      "Epoch [582/600], Loss: 0.5154\n",
      "Epoch [583/600], Loss: 0.5158\n",
      "Epoch [584/600], Loss: 0.5162\n",
      "Epoch [585/600], Loss: 0.5168\n",
      "Epoch [586/600], Loss: 0.5172\n",
      "Epoch [587/600], Loss: 0.5178\n",
      "Epoch [588/600], Loss: 0.5178\n",
      "Epoch [589/600], Loss: 0.5176\n",
      "Epoch [590/600], Loss: 0.5168\n",
      "Epoch [591/600], Loss: 0.5160\n",
      "Epoch [592/600], Loss: 0.5153\n",
      "Epoch [593/600], Loss: 0.5150\n",
      "Epoch [594/600], Loss: 0.5150\n",
      "Epoch [595/600], Loss: 0.5153\n",
      "Epoch [596/600], Loss: 0.5158\n",
      "Epoch [597/600], Loss: 0.5162\n",
      "Epoch [598/600], Loss: 0.5169\n",
      "Epoch [599/600], Loss: 0.5171\n",
      "Epoch [600/600], Loss: 0.5174\n",
      "Min loss: 0.5149622559547424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "losses = []\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = combined_features.shape[1]\n",
    "model = SimpleNN(input_dim)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model_state = None\n",
    "# Training loop\n",
    "epochs = 600\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    losses.append(loss.item())\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        best_model_state = model.state_dict()  # Save the model's state\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Min loss:\", min(losses))\n",
    "\n",
    "# After training, load the best model's state\n",
    "model.load_state_dict(best_model_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x159dd6680>]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7346\n",
      "Confusion Matrix:\n",
      " [[26514 15691]\n",
      " [ 6748 35585]]\n"
     ]
    }
   ],
   "source": [
    "# Convert test data to PyTorch tensors\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor)\n",
    "    predictions = (predictions > 0.5).float()  # Convert probabilities to binary predictions\n",
    "    accuracy = (predictions == y_test_tensor).float().mean()\n",
    "    print(f'Accuracy: {accuracy.item():.4f}')\n",
    "    # print confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_tensor, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model to /model\n",
    "\n",
    "torch.save(model.state_dict(), '/Users/gioelemonopoli/Documents/hack/Brainwave-Bandits/BrainwaveBandits.WinerR/mlbackend/model/model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### contrastive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming `combined_df` is your DataFrame with 'Harmonize' (Food Description) and wine features\n",
    "wine_features = combined_df[['Type', 'Elaborate', 'Body', 'Acidity', 'ABV']].values\n",
    "\n",
    "# Normalize the wine features\n",
    "scaler = StandardScaler()\n",
    "wine_features = scaler.fit_transform(wine_features)\n",
    "\n",
    "text_embeddings = np.load('data/text_embeddings.npy')\n",
    "\n",
    "# Now we have `text_embeddings` and `wine_features` as separate inputs\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train_text, X_test_text, X_train_wine, X_test_wine, y_train, y_test = train_test_split(\n",
    "    text_embeddings, wine_features, combined_df['pairing'], test_size=0.2, random_state=42,shuffle=True)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_text_tensor = torch.tensor(X_train_text, dtype=torch.float32)\n",
    "X_train_wine_tensor = torch.tensor(X_train_wine, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# You would also prepare the test tensors in the same way\n",
    "X_test_text_tensor = torch.tensor(X_test_text, dtype=torch.float32)\n",
    "X_test_wine_tensor = torch.tensor(X_test_wine, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, projection_dim,dropout=0.2):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, text_projection_head, wine_projection_head):\n",
    "        super(ContrastiveModel, self).__init__()\n",
    "        self.text_projection_head = text_projection_head\n",
    "        self.wine_projection_head = wine_projection_head\n",
    "\n",
    "    def forward(self, text_embedding, wine_input):\n",
    "        # Pass text embeddings through the text projection head\n",
    "        food_embedding = self.text_projection_head(text_embedding)\n",
    "\n",
    "        # Pass wine features through the wine projection head\n",
    "        wine_embedding = self.wine_projection_head(wine_input)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = F.cosine_similarity(food_embedding, wine_embedding, dim=1)\n",
    "        return similarity\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, similarity, label):\n",
    "        label = label.squeeze()\n",
    "        print(\"Positive similarity mean:\", similarity[label == 1].mean().item())\n",
    "        print(\"Negative similarity mean:\", similarity[label == 0].mean().item())\n",
    "        # Positive pairs should have similarity close to 1\n",
    "        pos_loss = (1 - similarity) * label\n",
    "        \n",
    "        # Negative pairs should have similarity close to 0 or less than margin\n",
    "        neg_loss = F.relu(similarity - self.margin) * (1 - label)\n",
    "        print(\"Positive loss mean:\", pos_loss.mean().item())\n",
    "        print(\"Negative loss mean:\", neg_loss.mean().item())\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = pos_loss.mean() + neg_loss.mean()\n",
    "        #loss = 0.5 * pos_loss.mean() + 1.5 * neg_loss.mean()\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dimensions\n",
    "text_embedding_dim = 384\n",
    "wine_embedding_dim = 5  # Example input dimension for wine\n",
    "projection_dim = 1024  # Dimension to which embeddings will be projected\n",
    "\n",
    "# Initialize models\n",
    "text_projection_head = ProjectionHead(embedding_dim=text_embedding_dim, projection_dim=projection_dim)\n",
    "wine_projection_head = ProjectionHead(embedding_dim=wine_embedding_dim, projection_dim=projection_dim)\n",
    "model = ContrastiveModel(text_projection_head, wine_projection_head)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = ContrastiveLoss(margin=0.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 256  # Adjust based on your memory and dataset size\n",
    "\n",
    "# Determine the number of batches\n",
    "num_batches = len(X_train_text_tensor) // batch_size\n",
    "\n",
    "num_epochs = 3\n",
    "losses = []\n",
    "steps = 0\n",
    "# Training loop, tqdm\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    \"\"\" if steps > 1000:\n",
    "        break \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # Shuffle the data at the start of each epoch (optional)\n",
    "    permutation = torch.randperm(X_train_text_tensor.size()[0])\n",
    "    X_train_text_tensor = X_train_text_tensor[permutation]\n",
    "    X_train_wine_tensor = X_train_wine_tensor[permutation]\n",
    "    y_train_tensor = y_train_tensor[permutation]\n",
    "    \n",
    "    for i in tqdm(range(num_batches),total=num_batches):\n",
    "        \n",
    "        # Create batches\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        batch_X_text = X_train_text_tensor[start_idx:end_idx]\n",
    "        \n",
    "        batch_X_wine = X_train_wine_tensor[start_idx:end_idx]\n",
    "        batch_y = y_train_tensor[start_idx:end_idx]\n",
    "\n",
    "        # Forward pass\n",
    "        similarity = model(batch_X_text, batch_X_wine)\n",
    "        loss = criterion(similarity, batch_y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        steps += 1\n",
    "\n",
    "        \"\"\" if steps > 1000:\n",
    "            break \"\"\"\n",
    "\n",
    "    \n",
    "    # Print the loss for this epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / num_batches:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 85/330 [00:00<00:00, 426.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive similarity mean: 0.8354385495185852\n",
      "Negative similarity mean: 0.4355548024177551\n",
      "Positive loss mean: 0.08613763749599457\n",
      "Negative loss mean: 0.2237648218870163\n",
      "Positive similarity mean: 0.8230441212654114\n",
      "Negative similarity mean: 0.4888809621334076\n",
      "Positive loss mean: 0.0857129767537117\n",
      "Negative loss mean: 0.2652973234653473\n",
      "Positive similarity mean: 0.880196213722229\n",
      "Negative similarity mean: 0.4592832326889038\n",
      "Positive loss mean: 0.0599018856883049\n",
      "Negative loss mean: 0.24460461735725403\n",
      "Positive similarity mean: 0.8086575865745544\n",
      "Negative similarity mean: 0.4664919078350067\n",
      "Positive loss mean: 0.09417636692523956\n",
      "Negative loss mean: 0.25460582971572876\n",
      "Positive similarity mean: 0.8389385342597961\n",
      "Negative similarity mean: 0.5312001705169678\n",
      "Positive loss mean: 0.0754975825548172\n",
      "Negative loss mean: 0.29372018575668335\n",
      "Positive similarity mean: 0.828246533870697\n",
      "Negative similarity mean: 0.4912249743938446\n",
      "Positive loss mean: 0.08856040239334106\n",
      "Negative loss mean: 0.2514994442462921\n",
      "Positive similarity mean: 0.7907333970069885\n",
      "Negative similarity mean: 0.48899221420288086\n",
      "Positive loss mean: 0.09972865134477615\n",
      "Negative loss mean: 0.2691821753978729\n",
      "Positive similarity mean: 0.8414021730422974\n",
      "Negative similarity mean: 0.48124775290489197\n",
      "Positive loss mean: 0.08115746825933456\n",
      "Negative loss mean: 0.24657806754112244\n",
      "Positive similarity mean: 0.8303000330924988\n",
      "Negative similarity mean: 0.48727571964263916\n",
      "Positive loss mean: 0.0835242047905922\n",
      "Negative loss mean: 0.26035866141319275\n",
      "Positive similarity mean: 0.8239808082580566\n",
      "Negative similarity mean: 0.5096957087516785\n",
      "Positive loss mean: 0.08732202649116516\n",
      "Negative loss mean: 0.2711663246154785\n",
      "Positive similarity mean: 0.7870020866394043\n",
      "Negative similarity mean: 0.4442653954029083\n",
      "Positive loss mean: 0.101506806910038\n",
      "Negative loss mean: 0.2476808726787567\n",
      "Positive similarity mean: 0.7884419560432434\n",
      "Negative similarity mean: 0.39762547612190247\n",
      "Positive loss mean: 0.10495263338088989\n",
      "Negative loss mean: 0.21630603075027466\n",
      "Positive similarity mean: 0.8204553723335266\n",
      "Negative similarity mean: 0.48048847913742065\n",
      "Positive loss mean: 0.09117504954338074\n",
      "Negative loss mean: 0.2488226592540741\n",
      "Positive similarity mean: 0.8955022096633911\n",
      "Negative similarity mean: 0.4645857810974121\n",
      "Positive loss mean: 0.04979974403977394\n",
      "Negative loss mean: 0.2595258355140686\n",
      "Positive similarity mean: 0.8112173676490784\n",
      "Negative similarity mean: 0.4575532376766205\n",
      "Positive loss mean: 0.08259238302707672\n",
      "Negative loss mean: 0.2784960865974426\n",
      "Positive similarity mean: 0.8161555528640747\n",
      "Negative similarity mean: 0.4158535897731781\n",
      "Positive loss mean: 0.09120411425828934\n",
      "Negative loss mean: 0.22978371381759644\n",
      "Positive similarity mean: 0.8432921767234802\n",
      "Negative similarity mean: 0.45742475986480713\n",
      "Positive loss mean: 0.07957824319601059\n",
      "Negative loss mean: 0.2426237016916275\n",
      "Positive similarity mean: 0.8266347050666809\n",
      "Negative similarity mean: 0.5040634274482727\n",
      "Positive loss mean: 0.09142310917377472\n",
      "Negative loss mean: 0.24660301208496094\n",
      "Positive similarity mean: 0.8412810564041138\n",
      "Negative similarity mean: 0.40533438324928284\n",
      "Positive loss mean: 0.08369939774274826\n",
      "Negative loss mean: 0.20680740475654602\n",
      "Positive similarity mean: 0.8287039995193481\n",
      "Negative similarity mean: 0.49979501962661743\n",
      "Positive loss mean: 0.09100104123353958\n",
      "Negative loss mean: 0.24748578667640686\n",
      "Positive similarity mean: 0.8143229484558105\n",
      "Negative similarity mean: 0.45932692289352417\n",
      "Positive loss mean: 0.08486019819974899\n",
      "Negative loss mean: 0.2710839509963989\n",
      "Positive similarity mean: 0.8513593077659607\n",
      "Negative similarity mean: 0.45626333355903625\n",
      "Positive loss mean: 0.08244913071393967\n",
      "Negative loss mean: 0.21769316494464874\n",
      "Positive similarity mean: 0.8637371063232422\n",
      "Negative similarity mean: 0.5159632563591003\n",
      "Positive loss mean: 0.06919600069522858\n",
      "Negative loss mean: 0.2660070061683655\n",
      "Positive similarity mean: 0.8312755823135376\n",
      "Negative similarity mean: 0.5039242506027222\n",
      "Positive loss mean: 0.08040772378444672\n",
      "Negative loss mean: 0.2789304852485657\n",
      "Positive similarity mean: 0.8240646719932556\n",
      "Negative similarity mean: 0.46196624636650085\n",
      "Positive loss mean: 0.08659319579601288\n",
      "Negative loss mean: 0.2467420995235443\n",
      "Positive similarity mean: 0.8624309301376343\n",
      "Negative similarity mean: 0.4507707953453064\n",
      "Positive loss mean: 0.06824717670679092\n",
      "Negative loss mean: 0.24097758531570435\n",
      "Positive similarity mean: 0.8266095519065857\n",
      "Negative similarity mean: 0.4295499324798584\n",
      "Positive loss mean: 0.08940447866916656\n",
      "Negative loss mean: 0.22311308979988098\n",
      "Positive similarity mean: 0.7843130826950073\n",
      "Negative similarity mean: 0.41720283031463623\n",
      "Positive loss mean: 0.10363079607486725\n",
      "Negative loss mean: 0.23426993191242218\n",
      "Positive similarity mean: 0.7941061854362488\n",
      "Negative similarity mean: 0.40341493487358093\n",
      "Positive loss mean: 0.10455543547868729\n",
      "Negative loss mean: 0.21514010429382324\n",
      "Positive similarity mean: 0.7829071283340454\n",
      "Negative similarity mean: 0.4724172353744507\n",
      "Positive loss mean: 0.10854645818471909\n",
      "Negative loss mean: 0.24890293180942535\n",
      "Positive similarity mean: 0.8389952182769775\n",
      "Negative similarity mean: 0.4245606064796448\n",
      "Positive loss mean: 0.07106852531433105\n",
      "Negative loss mean: 0.2552073895931244\n",
      "Positive similarity mean: 0.8541359901428223\n",
      "Negative similarity mean: 0.39469125866889954\n",
      "Positive loss mean: 0.06780396401882172\n",
      "Negative loss mean: 0.2340746819972992\n",
      "Positive similarity mean: 0.7785986661911011\n",
      "Negative similarity mean: 0.4700567126274109\n",
      "Positive loss mean: 0.1055116057395935\n",
      "Negative loss mean: 0.2583547532558441\n",
      "Positive similarity mean: 0.8141923546791077\n",
      "Negative similarity mean: 0.4863909184932709\n",
      "Positive loss mean: 0.08419406414031982\n",
      "Negative loss mean: 0.27974769473075867\n",
      "Positive similarity mean: 0.7849128842353821\n",
      "Negative similarity mean: 0.5077652335166931\n",
      "Positive loss mean: 0.11174443364143372\n",
      "Negative loss mean: 0.25561076402664185\n",
      "Positive similarity mean: 0.7895938158035278\n",
      "Negative similarity mean: 0.3880389332771301\n",
      "Positive loss mean: 0.11095639318227768\n",
      "Negative loss mean: 0.19962763786315918\n",
      "Positive similarity mean: 0.7694147229194641\n",
      "Negative similarity mean: 0.4603918492794037\n",
      "Positive loss mean: 0.10448392480611801\n",
      "Negative loss mean: 0.26843583583831787\n",
      "Positive similarity mean: 0.8007175326347351\n",
      "Negative similarity mean: 0.4451688528060913\n",
      "Positive loss mean: 0.10275505483150482\n",
      "Negative loss mean: 0.2295641154050827\n",
      "Positive similarity mean: 0.8249360918998718\n",
      "Negative similarity mean: 0.5018921494483948\n",
      "Positive loss mean: 0.09026734530925751\n",
      "Negative loss mean: 0.2561436593532562\n",
      "Positive similarity mean: 0.8153581023216248\n",
      "Negative similarity mean: 0.46195101737976074\n",
      "Positive loss mean: 0.09304220974445343\n",
      "Negative loss mean: 0.24315553903579712\n",
      "Positive similarity mean: 0.8514110445976257\n",
      "Negative similarity mean: 0.5578778982162476\n",
      "Positive loss mean: 0.08358128368854523\n",
      "Negative loss mean: 0.2522629201412201\n",
      "Positive similarity mean: 0.8191934823989868\n",
      "Negative similarity mean: 0.5575711727142334\n",
      "Positive loss mean: 0.09746601432561874\n",
      "Negative loss mean: 0.267255574464798\n",
      "Positive similarity mean: 0.7988558411598206\n",
      "Negative similarity mean: 0.4152439534664154\n",
      "Positive loss mean: 0.1037149429321289\n",
      "Negative loss mean: 0.21483425796031952\n",
      "Positive similarity mean: 0.8333552479743958\n",
      "Negative similarity mean: 0.4610474109649658\n",
      "Positive loss mean: 0.07811468094587326\n",
      "Negative loss mean: 0.26240649819374084\n",
      "Positive similarity mean: 0.8264345526695251\n",
      "Negative similarity mean: 0.503588855266571\n",
      "Positive loss mean: 0.09085068106651306\n",
      "Negative loss mean: 0.25299665331840515\n",
      "Positive similarity mean: 0.7680675983428955\n",
      "Negative similarity mean: 0.47394490242004395\n",
      "Positive loss mean: 0.12230811268091202\n",
      "Negative loss mean: 0.23834604024887085\n",
      "Positive similarity mean: 0.8466819524765015\n",
      "Negative similarity mean: 0.47552216053009033\n",
      "Positive loss mean: 0.06647776067256927\n",
      "Negative loss mean: 0.2869032919406891\n",
      "Positive similarity mean: 0.8081116080284119\n",
      "Negative similarity mean: 0.4870680272579193\n",
      "Positive loss mean: 0.08170247077941895\n",
      "Negative loss mean: 0.2943692207336426\n",
      "Positive similarity mean: 0.8488025665283203\n",
      "Negative similarity mean: 0.4184655249118805\n",
      "Positive loss mean: 0.07500805705785751\n",
      "Negative loss mean: 0.22834444046020508\n",
      "Positive similarity mean: 0.8407679200172424\n",
      "Negative similarity mean: 0.464400053024292\n",
      "Positive loss mean: 0.07775001972913742\n",
      "Negative loss mean: 0.25583359599113464\n",
      "Positive similarity mean: 0.8300232291221619\n",
      "Negative similarity mean: 0.4482339024543762\n",
      "Positive loss mean: 0.07702074944972992\n",
      "Negative loss mean: 0.2620673179626465\n",
      "Positive similarity mean: 0.8054224848747253\n",
      "Negative similarity mean: 0.49317479133605957\n",
      "Positive loss mean: 0.10032902657985687\n",
      "Negative loss mean: 0.2519087493419647\n",
      "Positive similarity mean: 0.8288567662239075\n",
      "Negative similarity mean: 0.4473450779914856\n",
      "Positive loss mean: 0.08624013513326645\n",
      "Negative loss mean: 0.23602783679962158\n",
      "Positive similarity mean: 0.843627393245697\n",
      "Negative similarity mean: 0.3843051791191101\n",
      "Positive loss mean: 0.08001882582902908\n",
      "Negative loss mean: 0.2088691145181656\n",
      "Positive similarity mean: 0.8166613578796387\n",
      "Negative similarity mean: 0.4251212477684021\n",
      "Positive loss mean: 0.09668249636888504\n",
      "Negative loss mean: 0.21363522112369537\n",
      "Positive similarity mean: 0.7810953855514526\n",
      "Negative similarity mean: 0.46816566586494446\n",
      "Positive loss mean: 0.11030739545822144\n",
      "Negative loss mean: 0.24611830711364746\n",
      "Positive similarity mean: 0.8275583386421204\n",
      "Negative similarity mean: 0.46425050497055054\n",
      "Positive loss mean: 0.08891522139310837\n",
      "Negative loss mean: 0.23823200166225433\n",
      "Positive similarity mean: 0.8174059391021729\n",
      "Negative similarity mean: 0.4890480041503906\n",
      "Positive loss mean: 0.08416442573070526\n",
      "Negative loss mean: 0.2766551077365875\n",
      "Positive similarity mean: 0.8250005841255188\n",
      "Negative similarity mean: 0.40865105390548706\n",
      "Positive loss mean: 0.08818331360816956\n",
      "Negative loss mean: 0.22130228579044342\n",
      "Positive similarity mean: 0.815899133682251\n",
      "Negative similarity mean: 0.5339257717132568\n",
      "Positive loss mean: 0.10211843997240067\n",
      "Negative loss mean: 0.2501577138900757\n",
      "Positive similarity mean: 0.8187659978866577\n",
      "Negative similarity mean: 0.4875430464744568\n",
      "Positive loss mean: 0.09557263553142548\n",
      "Negative loss mean: 0.24298343062400818\n",
      "Positive similarity mean: 0.8548723459243774\n",
      "Negative similarity mean: 0.4964098632335663\n",
      "Positive loss mean: 0.07086307555437088\n",
      "Negative loss mean: 0.26794272661209106\n",
      "Positive similarity mean: 0.8663861751556396\n",
      "Negative similarity mean: 0.3873114287853241\n",
      "Positive loss mean: 0.06628495454788208\n",
      "Negative loss mean: 0.2127753496170044\n",
      "Positive similarity mean: 0.8654992580413818\n",
      "Negative similarity mean: 0.4835359454154968\n",
      "Positive loss mean: 0.06987730413675308\n",
      "Negative loss mean: 0.24514976143836975\n",
      "Positive similarity mean: 0.8322297930717468\n",
      "Negative similarity mean: 0.46895819902420044\n",
      "Positive loss mean: 0.091094009578228\n",
      "Negative loss mean: 0.22445888817310333\n",
      "Positive similarity mean: 0.84725022315979\n",
      "Negative similarity mean: 0.4391668736934662\n",
      "Positive loss mean: 0.07219810783863068\n",
      "Negative loss mean: 0.24726833403110504\n",
      "Positive similarity mean: 0.8180213570594788\n",
      "Negative similarity mean: 0.5730820894241333\n",
      "Positive loss mean: 0.09525446593761444\n",
      "Negative loss mean: 0.2810189127922058\n",
      "Positive similarity mean: 0.824832558631897\n",
      "Negative similarity mean: 0.3982236087322235\n",
      "Positive loss mean: 0.08005699515342712\n",
      "Negative loss mean: 0.23471295833587646\n",
      "Positive similarity mean: 0.8353739976882935\n",
      "Negative similarity mean: 0.4175715446472168\n",
      "Positive loss mean: 0.08231297135353088\n",
      "Negative loss mean: 0.22522221505641937\n",
      "Positive similarity mean: 0.8245110511779785\n",
      "Negative similarity mean: 0.4998634159564972\n",
      "Positive loss mean: 0.0918574407696724\n",
      "Negative loss mean: 0.25281769037246704\n",
      "Positive similarity mean: 0.7490362524986267\n",
      "Negative similarity mean: 0.41354697942733765\n",
      "Positive loss mean: 0.12352123111486435\n",
      "Negative loss mean: 0.22845451533794403\n",
      "Positive similarity mean: 0.849112868309021\n",
      "Negative similarity mean: 0.4776889383792877\n",
      "Positive loss mean: 0.07662235200405121\n",
      "Negative loss mean: 0.2493044137954712\n",
      "Positive similarity mean: 0.8342122435569763\n",
      "Negative similarity mean: 0.3696556091308594\n",
      "Positive loss mean: 0.08030347526073456\n",
      "Negative loss mean: 0.20935732126235962\n",
      "Positive similarity mean: 0.8582287430763245\n",
      "Negative similarity mean: 0.5966935157775879\n",
      "Positive loss mean: 0.07476218044757843\n",
      "Negative loss mean: 0.2899740934371948\n",
      "Positive similarity mean: 0.8714492321014404\n",
      "Negative similarity mean: 0.4738170802593231\n",
      "Positive loss mean: 0.06779046356678009\n",
      "Negative loss mean: 0.23637229204177856\n",
      "Positive similarity mean: 0.806235134601593\n",
      "Negative similarity mean: 0.43295344710350037\n",
      "Positive loss mean: 0.10142379999160767\n",
      "Negative loss mean: 0.2224608063697815\n",
      "Positive similarity mean: 0.7993992567062378\n",
      "Negative similarity mean: 0.4590698778629303\n",
      "Positive loss mean: 0.1003003790974617\n",
      "Negative loss mean: 0.24325346946716309\n",
      "Positive similarity mean: 0.8087682723999023\n",
      "Negative similarity mean: 0.46407872438430786\n",
      "Positive loss mean: 0.1000978872179985\n",
      "Negative loss mean: 0.23595866560935974\n",
      "Positive similarity mean: 0.8471987843513489\n",
      "Negative similarity mean: 0.4387965798377991\n",
      "Positive loss mean: 0.0710287019610405\n",
      "Negative loss mean: 0.25248032808303833\n",
      "Positive similarity mean: 0.8030994534492493\n",
      "Negative similarity mean: 0.49155083298683167\n",
      "Positive loss mean: 0.0992194265127182\n",
      "Negative loss mean: 0.2584396302700043\n",
      "Positive similarity mean: 0.8339735865592957\n",
      "Negative similarity mean: 0.4004983901977539\n",
      "Positive loss mean: 0.08236465603113174\n",
      "Negative loss mean: 0.21725864708423615\n",
      "Positive similarity mean: 0.8200508952140808\n",
      "Negative similarity mean: 0.45389825105667114\n",
      "Positive loss mean: 0.0899745374917984\n",
      "Negative loss mean: 0.2399398684501648\n",
      "Positive similarity mean: 0.8384577035903931\n",
      "Negative similarity mean: 0.40896257758140564\n",
      "Positive loss mean: 0.07635396718978882\n",
      "Negative loss mean: 0.23397359251976013\n",
      "Positive similarity mean: 0.8077220916748047\n",
      "Negative similarity mean: 0.47611019015312195\n",
      "Positive loss mean: 0.08412155508995056\n",
      "Negative loss mean: 0.2833532691001892\n",
      "Positive similarity mean: 0.8353245854377747\n",
      "Negative similarity mean: 0.5003708004951477\n",
      "Positive loss mean: 0.08684053272008896\n",
      "Negative loss mean: 0.24941742420196533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 128/330 [00:00<00:00, 399.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive similarity mean: 0.8426262736320496\n",
      "Negative similarity mean: 0.5045866966247559\n",
      "Positive loss mean: 0.08299005031585693\n",
      "Negative loss mean: 0.2510795295238495\n",
      "Positive similarity mean: 0.8042410612106323\n",
      "Negative similarity mean: 0.5073191523551941\n",
      "Positive loss mean: 0.11011438816785812\n",
      "Negative loss mean: 0.23507747054100037\n",
      "Positive similarity mean: 0.7949197292327881\n",
      "Negative similarity mean: 0.4519844949245453\n",
      "Positive loss mean: 0.09372807294130325\n",
      "Negative loss mean: 0.26130303740501404\n",
      "Positive similarity mean: 0.824112057685852\n",
      "Negative similarity mean: 0.4189653992652893\n",
      "Positive loss mean: 0.0762639045715332\n",
      "Negative loss mean: 0.2509883940219879\n",
      "Positive similarity mean: 0.8418560028076172\n",
      "Negative similarity mean: 0.5580211281776428\n",
      "Positive loss mean: 0.07968972623348236\n",
      "Negative loss mean: 0.2877514362335205\n",
      "Positive similarity mean: 0.8635780215263367\n",
      "Negative similarity mean: 0.44631585478782654\n",
      "Positive loss mean: 0.07034260034561157\n",
      "Negative loss mean: 0.22653213143348694\n",
      "Positive similarity mean: 0.7814551591873169\n",
      "Negative similarity mean: 0.40952739119529724\n",
      "Positive loss mean: 0.0998818576335907\n",
      "Negative loss mean: 0.23879846930503845\n",
      "Positive similarity mean: 0.8540576100349426\n",
      "Negative similarity mean: 0.4760773479938507\n",
      "Positive loss mean: 0.07696183025836945\n",
      "Negative loss mean: 0.23759612441062927\n",
      "Positive similarity mean: 0.8318100571632385\n",
      "Negative similarity mean: 0.4400528073310852\n",
      "Positive loss mean: 0.0867229625582695\n",
      "Negative loss mean: 0.22920523583889008\n",
      "Positive similarity mean: 0.8410703539848328\n",
      "Negative similarity mean: 0.5299147367477417\n",
      "Positive loss mean: 0.07201497256755829\n",
      "Negative loss mean: 0.3084283769130707\n",
      "Positive similarity mean: 0.8475528359413147\n",
      "Negative similarity mean: 0.46130073070526123\n",
      "Positive loss mean: 0.07503253221511841\n",
      "Negative loss mean: 0.25262969732284546\n",
      "Positive similarity mean: 0.7926204204559326\n",
      "Negative similarity mean: 0.39653855562210083\n",
      "Positive loss mean: 0.10368981212377548\n",
      "Negative loss mean: 0.21571660041809082\n",
      "Positive similarity mean: 0.8146796822547913\n",
      "Negative similarity mean: 0.40753695368766785\n",
      "Positive loss mean: 0.0991753563284874\n",
      "Negative loss mean: 0.20334488153457642\n",
      "Positive similarity mean: 0.8152453303337097\n",
      "Negative similarity mean: 0.5091475248336792\n",
      "Positive loss mean: 0.10031607002019882\n",
      "Negative loss mean: 0.24500113725662231\n",
      "Positive similarity mean: 0.7524881958961487\n",
      "Negative similarity mean: 0.4273220896720886\n",
      "Positive loss mean: 0.11795488744974136\n",
      "Negative loss mean: 0.23985585570335388\n",
      "Positive similarity mean: 0.8713169097900391\n",
      "Negative similarity mean: 0.5034163594245911\n",
      "Positive loss mean: 0.065346859395504\n",
      "Negative loss mean: 0.2607507109642029\n",
      "Positive similarity mean: 0.8286001086235046\n",
      "Negative similarity mean: 0.5088434815406799\n",
      "Positive loss mean: 0.09038668870925903\n",
      "Negative loss mean: 0.2540605962276459\n",
      "Positive similarity mean: 0.8523315191268921\n",
      "Negative similarity mean: 0.4693291187286377\n",
      "Positive loss mean: 0.06921959668397903\n",
      "Negative loss mean: 0.2663467526435852\n",
      "Positive similarity mean: 0.8369701504707336\n",
      "Negative similarity mean: 0.4134770333766937\n",
      "Positive loss mean: 0.08406226336956024\n",
      "Negative loss mean: 0.21626916527748108\n",
      "Positive similarity mean: 0.8199143409729004\n",
      "Negative similarity mean: 0.4236105680465698\n",
      "Positive loss mean: 0.08933937549591064\n",
      "Negative loss mean: 0.22549140453338623\n",
      "Positive similarity mean: 0.8417927622795105\n",
      "Negative similarity mean: 0.3760835826396942\n",
      "Positive loss mean: 0.08219362050294876\n",
      "Negative loss mean: 0.19544154405593872\n",
      "Positive similarity mean: 0.8206114172935486\n",
      "Negative similarity mean: 0.46039724349975586\n",
      "Positive loss mean: 0.08338767290115356\n",
      "Negative loss mean: 0.26081451773643494\n",
      "Positive similarity mean: 0.8509756326675415\n",
      "Negative similarity mean: 0.4815480709075928\n",
      "Positive loss mean: 0.07684071362018585\n",
      "Negative loss mean: 0.2439367175102234\n",
      "Positive similarity mean: 0.7529615759849548\n",
      "Negative similarity mean: 0.4391714334487915\n",
      "Positive loss mean: 0.1302742063999176\n",
      "Negative loss mean: 0.22229525446891785\n",
      "Positive similarity mean: 0.8257468938827515\n",
      "Negative similarity mean: 0.47242218255996704\n",
      "Positive loss mean: 0.09733668714761734\n",
      "Negative loss mean: 0.22061309218406677\n",
      "Positive similarity mean: 0.7917689085006714\n",
      "Negative similarity mean: 0.3941137492656708\n",
      "Positive loss mean: 0.10980936139822006\n",
      "Negative loss mean: 0.20308220386505127\n",
      "Positive similarity mean: 0.7932221293449402\n",
      "Negative similarity mean: 0.3778184950351715\n",
      "Positive loss mean: 0.10338891297578812\n",
      "Negative loss mean: 0.21232225000858307\n",
      "Positive similarity mean: 0.8283904194831848\n",
      "Negative similarity mean: 0.46709731221199036\n",
      "Positive loss mean: 0.08312340080738068\n",
      "Negative loss mean: 0.2567458748817444\n",
      "Positive similarity mean: 0.8249325752258301\n",
      "Negative similarity mean: 0.399696946144104\n",
      "Positive loss mean: 0.09642389416694641\n",
      "Negative loss mean: 0.19277094304561615\n",
      "Positive similarity mean: 0.8433183431625366\n",
      "Negative similarity mean: 0.4970106780529022\n",
      "Positive loss mean: 0.078952856361866\n",
      "Negative loss mean: 0.25887417793273926\n",
      "Positive similarity mean: 0.8726276159286499\n",
      "Negative similarity mean: 0.4443013668060303\n",
      "Positive loss mean: 0.06318861246109009\n",
      "Negative loss mean: 0.24176962673664093\n",
      "Positive similarity mean: 0.8756880760192871\n",
      "Negative similarity mean: 0.49901890754699707\n",
      "Positive loss mean: 0.06361272186040878\n",
      "Negative loss mean: 0.26388129591941833\n",
      "Positive similarity mean: 0.8390464186668396\n",
      "Negative similarity mean: 0.3984067440032959\n",
      "Positive loss mean: 0.08739277720451355\n",
      "Negative loss mean: 0.19920165836811066\n",
      "Positive similarity mean: 0.8498150706291199\n",
      "Negative similarity mean: 0.48319292068481445\n",
      "Positive loss mean: 0.08330570161342621\n",
      "Negative loss mean: 0.2297796607017517\n",
      "Positive similarity mean: 0.8502088785171509\n",
      "Negative similarity mean: 0.40273818373680115\n",
      "Positive loss mean: 0.07489555329084396\n",
      "Negative loss mean: 0.22162535786628723\n",
      "Positive similarity mean: 0.8231034874916077\n",
      "Negative similarity mean: 0.4849884808063507\n",
      "Positive loss mean: 0.08568421751260757\n",
      "Negative loss mean: 0.26627156138420105\n",
      "Positive similarity mean: 0.8011999130249023\n",
      "Negative similarity mean: 0.4417138397693634\n",
      "Positive loss mean: 0.09474065154790878\n",
      "Negative loss mean: 0.24667596817016602\n",
      "Positive similarity mean: 0.8324240446090698\n",
      "Negative similarity mean: 0.3988364338874817\n",
      "Positive loss mean: 0.08051499724388123\n",
      "Negative loss mean: 0.22573384642601013\n",
      "Positive similarity mean: 0.8544288277626038\n",
      "Negative similarity mean: 0.4320448040962219\n",
      "Positive loss mean: 0.07562877237796783\n",
      "Negative loss mean: 0.21977855265140533\n",
      "Positive similarity mean: 0.7954508662223816\n",
      "Negative similarity mean: 0.43458300828933716\n",
      "Positive loss mean: 0.09028928726911545\n",
      "Negative loss mean: 0.2583438754081726\n",
      "Positive similarity mean: 0.8133554458618164\n",
      "Negative similarity mean: 0.395442932844162\n",
      "Positive loss mean: 0.0933222770690918\n",
      "Negative loss mean: 0.21208900213241577\n",
      "Positive similarity mean: 0.8595467805862427\n",
      "Negative similarity mean: 0.492888480424881\n",
      "Positive loss mean: 0.07955357432365417\n",
      "Negative loss mean: 0.22660374641418457\n",
      "Positive similarity mean: 0.8437424898147583\n",
      "Negative similarity mean: 0.4468916654586792\n",
      "Positive loss mean: 0.07751834392547607\n",
      "Negative loss mean: 0.24047648906707764\n",
      "Positive similarity mean: 0.7856258153915405\n",
      "Negative similarity mean: 0.444199800491333\n",
      "Positive loss mean: 0.11137406527996063\n",
      "Negative loss mean: 0.22831209003925323\n",
      "Positive similarity mean: 0.8280026316642761\n",
      "Negative similarity mean: 0.4494023621082306\n",
      "Positive loss mean: 0.08263935893774033\n",
      "Negative loss mean: 0.24524646997451782\n",
      "Positive similarity mean: 0.8448942303657532\n",
      "Negative similarity mean: 0.4904897212982178\n",
      "Positive loss mean: 0.06725290417671204\n",
      "Negative loss mean: 0.2897854149341583\n",
      "Positive similarity mean: 0.795585036277771\n",
      "Negative similarity mean: 0.43681415915489197\n",
      "Positive loss mean: 0.10939393192529678\n",
      "Negative loss mean: 0.21657037734985352\n",
      "Positive similarity mean: 0.7795189023017883\n",
      "Negative similarity mean: 0.44656798243522644\n",
      "Positive loss mean: 0.11282433569431305\n",
      "Negative loss mean: 0.2386198788881302\n",
      "Positive similarity mean: 0.8456593155860901\n",
      "Negative similarity mean: 0.37354138493537903\n",
      "Positive loss mean: 0.08259639889001846\n",
      "Negative loss mean: 0.19203704595565796\n",
      "Positive similarity mean: 0.8551713228225708\n",
      "Negative similarity mean: 0.366512656211853\n",
      "Positive loss mean: 0.07750596106052399\n",
      "Negative loss mean: 0.18738889694213867\n",
      "Positive similarity mean: 0.862469494342804\n",
      "Negative similarity mean: 0.4296186864376068\n",
      "Positive loss mean: 0.06178131699562073\n",
      "Negative loss mean: 0.2544347047805786\n",
      "Positive similarity mean: 0.7959028482437134\n",
      "Negative similarity mean: 0.5322161912918091\n",
      "Positive loss mean: 0.09646778553724289\n",
      "Negative loss mean: 0.29367759823799133\n",
      "Positive similarity mean: 0.8977940678596497\n",
      "Negative similarity mean: 0.4763360917568207\n",
      "Positive loss mean: 0.05030447617173195\n",
      "Negative loss mean: 0.25898829102516174\n",
      "Positive similarity mean: 0.772580087184906\n",
      "Negative similarity mean: 0.46466004848480225\n",
      "Positive loss mean: 0.10482635349035263\n",
      "Negative loss mean: 0.2658551037311554\n",
      "Positive similarity mean: 0.8980649709701538\n",
      "Negative similarity mean: 0.4549078941345215\n",
      "Positive loss mean: 0.0489765927195549\n",
      "Negative loss mean: 0.2525108754634857\n",
      "Positive similarity mean: 0.854907214641571\n",
      "Negative similarity mean: 0.4593724012374878\n",
      "Positive loss mean: 0.07254637032747269\n",
      "Negative loss mean: 0.24512718617916107\n",
      "Positive similarity mean: 0.8400517106056213\n",
      "Negative similarity mean: 0.46482470631599426\n",
      "Positive loss mean: 0.06560380756855011\n",
      "Negative loss mean: 0.29392117261886597\n",
      "Positive similarity mean: 0.8293496370315552\n",
      "Negative similarity mean: 0.5060304403305054\n",
      "Positive loss mean: 0.09199125319719315\n",
      "Negative loss mean: 0.24636834859848022\n",
      "Positive similarity mean: 0.8724510669708252\n",
      "Negative similarity mean: 0.4489539563655853\n",
      "Positive loss mean: 0.06028677150607109\n",
      "Negative loss mean: 0.2539338767528534\n",
      "Positive similarity mean: 0.8199048042297363\n",
      "Negative similarity mean: 0.4287797212600708\n",
      "Positive loss mean: 0.09356505423784256\n",
      "Negative loss mean: 0.22348381578922272\n",
      "Positive similarity mean: 0.8280636072158813\n",
      "Negative similarity mean: 0.47953662276268005\n",
      "Positive loss mean: 0.08596824109554291\n",
      "Negative loss mean: 0.25534629821777344\n",
      "Positive similarity mean: 0.765129804611206\n",
      "Negative similarity mean: 0.4407148063182831\n",
      "Positive loss mean: 0.1293621063232422\n",
      "Negative loss mean: 0.21505580842494965\n",
      "Positive similarity mean: 0.7837241291999817\n",
      "Negative similarity mean: 0.4102380573749542\n",
      "Positive loss mean: 0.10391376912593842\n",
      "Negative loss mean: 0.23080596327781677\n",
      "Positive similarity mean: 0.8294755816459656\n",
      "Negative similarity mean: 0.38734495639801025\n",
      "Positive loss mean: 0.09125722944736481\n",
      "Negative loss mean: 0.19780243933200836\n",
      "Positive similarity mean: 0.8179059624671936\n",
      "Negative similarity mean: 0.39793023467063904\n",
      "Positive loss mean: 0.09318093955516815\n",
      "Negative loss mean: 0.2136150300502777\n",
      "Positive similarity mean: 0.7983030676841736\n",
      "Negative similarity mean: 0.41314342617988586\n",
      "Positive loss mean: 0.09769690781831741\n",
      "Negative loss mean: 0.22925274074077606\n",
      "Positive similarity mean: 0.8134908080101013\n",
      "Negative similarity mean: 0.42449676990509033\n",
      "Positive loss mean: 0.09471173584461212\n",
      "Negative loss mean: 0.22797609865665436\n",
      "Positive similarity mean: 0.85453200340271\n",
      "Negative similarity mean: 0.35989853739738464\n",
      "Positive loss mean: 0.0738704651594162\n",
      "Negative loss mean: 0.1957099735736847\n",
      "Positive similarity mean: 0.8491529822349548\n",
      "Negative similarity mean: 0.4613446295261383\n",
      "Positive loss mean: 0.08013748377561569\n",
      "Negative loss mean: 0.23122070729732513\n",
      "Positive similarity mean: 0.7785913348197937\n",
      "Negative similarity mean: 0.5103206634521484\n",
      "Positive loss mean: 0.10551506280899048\n",
      "Negative loss mean: 0.28553831577301025\n",
      "Positive similarity mean: 0.8197339773178101\n",
      "Negative similarity mean: 0.3996814489364624\n",
      "Positive loss mean: 0.08872465044260025\n",
      "Negative loss mean: 0.2216280698776245\n",
      "Positive similarity mean: 0.8251135349273682\n",
      "Negative similarity mean: 0.545113205909729\n",
      "Positive loss mean: 0.0840274840593338\n",
      "Negative loss mean: 0.2950071096420288\n",
      "Positive similarity mean: 0.8148708343505859\n",
      "Negative similarity mean: 0.47678056359291077\n",
      "Positive loss mean: 0.09618036448955536\n",
      "Negative loss mean: 0.24327611923217773\n",
      "Positive similarity mean: 0.841647744178772\n",
      "Negative similarity mean: 0.48233866691589355\n",
      "Positive loss mean: 0.07917613536119461\n",
      "Negative loss mean: 0.2535005807876587\n",
      "Positive similarity mean: 0.8614615797996521\n",
      "Negative similarity mean: 0.4893341064453125\n",
      "Positive loss mean: 0.07143387198448181\n",
      "Negative loss mean: 0.2527935206890106\n",
      "Positive similarity mean: 0.8233340978622437\n",
      "Negative similarity mean: 0.43120288848876953\n",
      "Positive loss mean: 0.08281214535236359\n",
      "Negative loss mean: 0.2466648519039154\n",
      "Positive similarity mean: 0.8366137146949768\n",
      "Negative similarity mean: 0.468488872051239\n",
      "Positive loss mean: 0.08424602448940277\n",
      "Negative loss mean: 0.24239572882652283\n",
      "Positive similarity mean: 0.8361139297485352\n",
      "Negative similarity mean: 0.37566420435905457\n",
      "Positive loss mean: 0.08514394611120224\n",
      "Negative loss mean: 0.19590136408805847\n",
      "Positive similarity mean: 0.8217014670372009\n",
      "Negative similarity mean: 0.4908599853515625\n",
      "Positive loss mean: 0.09750700742006302\n",
      "Negative loss mean: 0.23344773054122925\n",
      "Positive similarity mean: 0.819938063621521\n",
      "Negative similarity mean: 0.41368916630744934\n",
      "Positive loss mean: 0.0914376974105835\n",
      "Negative loss mean: 0.2210523635149002\n",
      "Positive similarity mean: 0.8273249268531799\n",
      "Negative similarity mean: 0.36312296986579895\n",
      "Positive loss mean: 0.08836106956005096\n",
      "Negative loss mean: 0.19778132438659668\n",
      "Positive similarity mean: 0.8209291100502014\n",
      "Negative similarity mean: 0.45911216735839844\n",
      "Positive loss mean: 0.08883596211671829\n",
      "Negative loss mean: 0.2435166835784912\n",
      "Positive similarity mean: 0.8103281259536743\n",
      "Negative similarity mean: 0.4499386250972748\n",
      "Positive loss mean: 0.08520416915416718\n",
      "Negative loss mean: 0.2642112374305725\n",
      "Positive similarity mean: 0.8560360670089722\n",
      "Negative similarity mean: 0.4072093963623047\n",
      "Positive loss mean: 0.06692073494195938\n",
      "Negative loss mean: 0.233592689037323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 221/330 [00:00<00:00, 436.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive similarity mean: 0.7789681553840637\n",
      "Negative similarity mean: 0.4597480893135071\n",
      "Positive loss mean: 0.11224273592233658\n",
      "Negative loss mean: 0.2425503432750702\n",
      "Positive similarity mean: 0.8593587875366211\n",
      "Negative similarity mean: 0.44361141324043274\n",
      "Positive loss mean: 0.07691320776939392\n",
      "Negative loss mean: 0.21291984617710114\n",
      "Positive similarity mean: 0.762859046459198\n",
      "Negative similarity mean: 0.4342186450958252\n",
      "Positive loss mean: 0.11671781539916992\n",
      "Negative loss mean: 0.23977130651474\n",
      "Positive similarity mean: 0.865969181060791\n",
      "Negative similarity mean: 0.4599086046218872\n",
      "Positive loss mean: 0.06753896176815033\n",
      "Negative loss mean: 0.2411797046661377\n",
      "Positive similarity mean: 0.7998627424240112\n",
      "Negative similarity mean: 0.46014127135276794\n",
      "Positive loss mean: 0.10397754609584808\n",
      "Negative loss mean: 0.2375296950340271\n",
      "Positive similarity mean: 0.7946739792823792\n",
      "Negative similarity mean: 0.5096153020858765\n",
      "Positive loss mean: 0.09865275025367737\n",
      "Negative loss mean: 0.27699315547943115\n",
      "Positive similarity mean: 0.815170168876648\n",
      "Negative similarity mean: 0.4570417106151581\n",
      "Positive loss mean: 0.09530287981033325\n",
      "Negative loss mean: 0.2370108962059021\n",
      "Positive similarity mean: 0.8412079811096191\n",
      "Negative similarity mean: 0.4579422175884247\n",
      "Positive loss mean: 0.06947150826454163\n",
      "Negative loss mean: 0.2752341032028198\n",
      "Positive similarity mean: 0.8040634989738464\n",
      "Negative similarity mean: 0.46753016114234924\n",
      "Positive loss mean: 0.09720288962125778\n",
      "Negative loss mean: 0.25173527002334595\n",
      "Positive similarity mean: 0.862627387046814\n",
      "Negative similarity mean: 0.48058751225471497\n",
      "Positive loss mean: 0.06439340859651566\n",
      "Negative loss mean: 0.271171897649765\n",
      "Positive similarity mean: 0.8649730086326599\n",
      "Negative similarity mean: 0.4617440700531006\n",
      "Positive loss mean: 0.06804094463586807\n",
      "Negative loss mean: 0.24746611714363098\n",
      "Positive similarity mean: 0.8240775465965271\n",
      "Negative similarity mean: 0.5738297700881958\n",
      "Positive loss mean: 0.09826920926570892\n",
      "Negative loss mean: 0.26454460620880127\n",
      "Positive similarity mean: 0.8150302767753601\n",
      "Negative similarity mean: 0.4758853018283844\n",
      "Positive loss mean: 0.08236932754516602\n",
      "Negative loss mean: 0.2814010977745056\n",
      "Positive similarity mean: 0.8040341138839722\n",
      "Negative similarity mean: 0.45897382497787476\n",
      "Positive loss mean: 0.09798295050859451\n",
      "Negative loss mean: 0.24376201629638672\n",
      "Positive similarity mean: 0.8587369322776794\n",
      "Negative similarity mean: 0.5182396173477173\n",
      "Positive loss mean: 0.064561627805233\n",
      "Negative loss mean: 0.2946377694606781\n",
      "Positive similarity mean: 0.8479176759719849\n",
      "Negative similarity mean: 0.4599084258079529\n",
      "Positive loss mean: 0.0760410949587822\n",
      "Negative loss mean: 0.24096746742725372\n",
      "Positive similarity mean: 0.873305082321167\n",
      "Negative similarity mean: 0.4275038242340088\n",
      "Positive loss mean: 0.06483211368322372\n",
      "Negative loss mean: 0.22475938498973846\n",
      "Positive similarity mean: 0.8413547277450562\n",
      "Negative similarity mean: 0.43641793727874756\n",
      "Positive loss mean: 0.07188611477613449\n",
      "Negative loss mean: 0.25383907556533813\n",
      "Positive similarity mean: 0.8005455732345581\n",
      "Negative similarity mean: 0.4677600562572479\n",
      "Positive loss mean: 0.1051810085773468\n",
      "Negative loss mean: 0.23403000831604004\n",
      "Positive similarity mean: 0.7405744194984436\n",
      "Negative similarity mean: 0.4741063416004181\n",
      "Positive loss mean: 0.126672625541687\n",
      "Negative loss mean: 0.2564379572868347\n",
      "Positive similarity mean: 0.8949741125106812\n",
      "Negative similarity mean: 0.525885283946991\n",
      "Positive loss mean: 0.05415397882461548\n",
      "Negative loss mean: 0.26761507987976074\n",
      "Positive similarity mean: 0.81043541431427\n",
      "Negative similarity mean: 0.5204612016677856\n",
      "Positive loss mean: 0.08885839581489563\n",
      "Negative loss mean: 0.2887015640735626\n",
      "Positive similarity mean: 0.79461669921875\n",
      "Negative similarity mean: 0.4277426302433014\n",
      "Positive loss mean: 0.10349391400814056\n",
      "Negative loss mean: 0.2280523180961609\n",
      "Positive similarity mean: 0.8484779000282288\n",
      "Negative similarity mean: 0.41250887513160706\n",
      "Positive loss mean: 0.07694481313228607\n",
      "Negative loss mean: 0.2199384570121765\n",
      "Positive similarity mean: 0.7872155904769897\n",
      "Negative similarity mean: 0.530597984790802\n",
      "Positive loss mean: 0.10472981631755829\n",
      "Negative loss mean: 0.2793470323085785\n",
      "Positive similarity mean: 0.8209388256072998\n",
      "Negative similarity mean: 0.4313189685344696\n",
      "Positive loss mean: 0.08673270046710968\n",
      "Negative loss mean: 0.23992326855659485\n",
      "Positive similarity mean: 0.8176171183586121\n",
      "Negative similarity mean: 0.46831321716308594\n",
      "Positive loss mean: 0.08905411511659622\n",
      "Negative loss mean: 0.25419455766677856\n",
      "Positive similarity mean: 0.8350725173950195\n",
      "Negative similarity mean: 0.4535748362541199\n",
      "Positive loss mean: 0.07730977237224579\n",
      "Negative loss mean: 0.25846824049949646\n",
      "Positive similarity mean: 0.8686460852622986\n",
      "Negative similarity mean: 0.4625305235385895\n",
      "Positive loss mean: 0.06619004905223846\n",
      "Negative loss mean: 0.24471688270568848\n",
      "Positive similarity mean: 0.8319247961044312\n",
      "Negative similarity mean: 0.4876474142074585\n",
      "Positive loss mean: 0.08535071462392807\n",
      "Negative loss mean: 0.2513072192668915\n",
      "Positive similarity mean: 0.8357399702072144\n",
      "Negative similarity mean: 0.44351881742477417\n",
      "Positive loss mean: 0.0795634537935257\n",
      "Negative loss mean: 0.24417677521705627\n",
      "Positive similarity mean: 0.8209614157676697\n",
      "Negative similarity mean: 0.45767873525619507\n",
      "Positive loss mean: 0.08532308787107468\n",
      "Negative loss mean: 0.257487028837204\n",
      "Positive similarity mean: 0.8084508180618286\n",
      "Negative similarity mean: 0.47306519746780396\n",
      "Positive loss mean: 0.09876751899719238\n",
      "Negative loss mean: 0.24008865654468536\n",
      "Positive similarity mean: 0.8300880193710327\n",
      "Negative similarity mean: 0.4912409782409668\n",
      "Positive loss mean: 0.08628343045711517\n",
      "Negative loss mean: 0.2553870975971222\n",
      "Positive similarity mean: 0.8577739000320435\n",
      "Negative similarity mean: 0.4459649622440338\n",
      "Positive loss mean: 0.06777966767549515\n",
      "Negative loss mean: 0.25035348534584045\n",
      "Positive similarity mean: 0.8640145659446716\n",
      "Negative similarity mean: 0.49113669991493225\n",
      "Positive loss mean: 0.06480558216571808\n",
      "Negative loss mean: 0.26888760924339294\n",
      "Positive similarity mean: 0.8561016321182251\n",
      "Negative similarity mean: 0.3945581316947937\n",
      "Positive loss mean: 0.07194921374320984\n",
      "Negative loss mean: 0.21509727835655212\n",
      "Positive similarity mean: 0.8582388162612915\n",
      "Negative similarity mean: 0.44923537969589233\n",
      "Positive loss mean: 0.07143434882164001\n",
      "Negative loss mean: 0.24124157428741455\n",
      "Positive similarity mean: 0.8365199565887451\n",
      "Negative similarity mean: 0.46706485748291016\n",
      "Positive loss mean: 0.09195747971534729\n",
      "Negative loss mean: 0.2162472903728485\n",
      "Positive similarity mean: 0.858124315738678\n",
      "Negative similarity mean: 0.42552053928375244\n",
      "Positive loss mean: 0.07370886206626892\n",
      "Negative loss mean: 0.2192944586277008\n",
      "Positive similarity mean: 0.8634786009788513\n",
      "Negative similarity mean: 0.4545399844646454\n",
      "Positive loss mean: 0.06399441510438919\n",
      "Negative loss mean: 0.25895917415618896\n",
      "Positive similarity mean: 0.7986849546432495\n",
      "Negative similarity mean: 0.5552647113800049\n",
      "Positive loss mean: 0.09593921154737473\n",
      "Negative loss mean: 0.29953548312187195\n",
      "Positive similarity mean: 0.7991499304771423\n",
      "Negative similarity mean: 0.5540288686752319\n",
      "Positive loss mean: 0.09728678315877914\n",
      "Negative loss mean: 0.29482635855674744\n",
      "Positive similarity mean: 0.8386420011520386\n",
      "Negative similarity mean: 0.47300612926483154\n",
      "Positive loss mean: 0.08572142571210861\n",
      "Negative loss mean: 0.23591287434101105\n",
      "Positive similarity mean: 0.8169031143188477\n",
      "Negative similarity mean: 0.38560307025909424\n",
      "Positive loss mean: 0.08940280228853226\n",
      "Negative loss mean: 0.21731449663639069\n",
      "Positive similarity mean: 0.8062752485275269\n",
      "Negative similarity mean: 0.46904873847961426\n",
      "Positive loss mean: 0.10140281170606613\n",
      "Negative loss mean: 0.23708941042423248\n",
      "Positive similarity mean: 0.8707147240638733\n",
      "Negative similarity mean: 0.3984573185443878\n",
      "Positive loss mean: 0.05807735025882721\n",
      "Negative loss mean: 0.23931601643562317\n",
      "Positive similarity mean: 0.8231689929962158\n",
      "Negative similarity mean: 0.5240269899368286\n",
      "Positive loss mean: 0.09117849171161652\n",
      "Negative loss mean: 0.26584410667419434\n",
      "Positive similarity mean: 0.8251584768295288\n",
      "Negative similarity mean: 0.49954119324684143\n",
      "Positive loss mean: 0.08605478703975677\n",
      "Negative loss mean: 0.26784205436706543\n",
      "Positive similarity mean: 0.8405060172080994\n",
      "Negative similarity mean: 0.4120611548423767\n",
      "Positive loss mean: 0.08348512649536133\n",
      "Negative loss mean: 0.21100853383541107\n",
      "Positive similarity mean: 0.8838579654693604\n",
      "Negative similarity mean: 0.43798133730888367\n",
      "Positive loss mean: 0.05534891039133072\n",
      "Negative loss mean: 0.24771146476268768\n",
      "Positive similarity mean: 0.8507447838783264\n",
      "Negative similarity mean: 0.3813423812389374\n",
      "Positive loss mean: 0.07112942636013031\n",
      "Negative loss mean: 0.2172870635986328\n",
      "Positive similarity mean: 0.8278287649154663\n",
      "Negative similarity mean: 0.5490809679031372\n",
      "Positive loss mean: 0.09550122171640396\n",
      "Negative loss mean: 0.25581273436546326\n",
      "Positive similarity mean: 0.8346973657608032\n",
      "Negative similarity mean: 0.4881896376609802\n",
      "Positive loss mean: 0.07813134789466858\n",
      "Negative loss mean: 0.27166497707366943\n",
      "Positive similarity mean: 0.8741527795791626\n",
      "Negative similarity mean: 0.45553889870643616\n",
      "Positive loss mean: 0.06833109259605408\n",
      "Negative loss mean: 0.2221851646900177\n",
      "Positive similarity mean: 0.8490509986877441\n",
      "Negative similarity mean: 0.4724179804325104\n",
      "Positive loss mean: 0.07960202544927597\n",
      "Negative loss mean: 0.23572151362895966\n",
      "Positive similarity mean: 0.8154130578041077\n",
      "Negative similarity mean: 0.4961186349391937\n",
      "Positive loss mean: 0.08580407500267029\n",
      "Negative loss mean: 0.27935242652893066\n",
      "Positive similarity mean: 0.8132327198982239\n",
      "Negative similarity mean: 0.4695533812046051\n",
      "Positive loss mean: 0.09922013431787491\n",
      "Negative loss mean: 0.23469161987304688\n",
      "Positive similarity mean: 0.8254967331886292\n",
      "Negative similarity mean: 0.5017509460449219\n",
      "Positive loss mean: 0.09202326089143753\n",
      "Negative loss mean: 0.25040578842163086\n",
      "Positive similarity mean: 0.777791440486908\n",
      "Negative similarity mean: 0.42722582817077637\n",
      "Positive loss mean: 0.11457625776529312\n",
      "Negative loss mean: 0.2199353128671646\n",
      "Positive similarity mean: 0.8264620900154114\n",
      "Negative similarity mean: 0.41419124603271484\n",
      "Positive loss mean: 0.08202377706766129\n",
      "Negative loss mean: 0.23314787447452545\n",
      "Positive similarity mean: 0.846781849861145\n",
      "Negative similarity mean: 0.4051799178123474\n",
      "Positive loss mean: 0.07241953164339066\n",
      "Negative loss mean: 0.23036906123161316\n",
      "Positive similarity mean: 0.8310639262199402\n",
      "Negative similarity mean: 0.5211060047149658\n",
      "Positive loss mean: 0.08182843774557114\n",
      "Negative loss mean: 0.28358742594718933\n",
      "Positive similarity mean: 0.8102440237998962\n",
      "Negative similarity mean: 0.5115410685539246\n",
      "Positive loss mean: 0.09487796574831009\n",
      "Negative loss mean: 0.26851412653923035\n",
      "Positive similarity mean: 0.8388075828552246\n",
      "Negative similarity mean: 0.45602038502693176\n",
      "Positive loss mean: 0.07241062819957733\n",
      "Negative loss mean: 0.26297882199287415\n",
      "Positive similarity mean: 0.8258699178695679\n",
      "Negative similarity mean: 0.3993871510028839\n",
      "Positive loss mean: 0.08026308566331863\n",
      "Negative loss mean: 0.23629513382911682\n",
      "Positive similarity mean: 0.8416853547096252\n",
      "Negative similarity mean: 0.5007888078689575\n",
      "Positive loss mean: 0.0791572704911232\n",
      "Negative loss mean: 0.2637200653553009\n",
      "Positive similarity mean: 0.8527712225914001\n",
      "Negative similarity mean: 0.4972929358482361\n",
      "Positive loss mean: 0.07476463168859482\n",
      "Negative loss mean: 0.25705450773239136\n",
      "Positive similarity mean: 0.8396955728530884\n",
      "Negative similarity mean: 0.3922770023345947\n",
      "Positive loss mean: 0.07952605187892914\n",
      "Negative loss mean: 0.21366816759109497\n",
      "Positive similarity mean: 0.8399378657341003\n",
      "Negative similarity mean: 0.4895108938217163\n",
      "Positive loss mean: 0.07252815365791321\n",
      "Negative loss mean: 0.2835865020751953\n",
      "Positive similarity mean: 0.8340383172035217\n",
      "Negative similarity mean: 0.4728489816188812\n",
      "Positive loss mean: 0.07455310225486755\n",
      "Negative loss mean: 0.2783361077308655\n",
      "Positive similarity mean: 0.8225059509277344\n",
      "Negative similarity mean: 0.3640090525150299\n",
      "Positive loss mean: 0.08458703756332397\n",
      "Negative loss mean: 0.20835387706756592\n",
      "Positive similarity mean: 0.8356840014457703\n",
      "Negative similarity mean: 0.4798699915409088\n",
      "Positive loss mean: 0.08023242652416229\n",
      "Negative loss mean: 0.2599424421787262\n",
      "Positive similarity mean: 0.8492907285690308\n",
      "Negative similarity mean: 0.3969651460647583\n",
      "Positive loss mean: 0.06946753710508347\n",
      "Negative loss mean: 0.23172789812088013\n",
      "Positive similarity mean: 0.8316184282302856\n",
      "Negative similarity mean: 0.5094835162162781\n",
      "Positive loss mean: 0.09734557569026947\n",
      "Negative loss mean: 0.2257639765739441\n",
      "Positive similarity mean: 0.8025546669960022\n",
      "Negative similarity mean: 0.4455576241016388\n",
      "Positive loss mean: 0.10026517510414124\n",
      "Negative loss mean: 0.23686091601848602\n",
      "Positive similarity mean: 0.8617849349975586\n",
      "Negative similarity mean: 0.4523875117301941\n",
      "Positive loss mean: 0.06586810946464539\n",
      "Negative loss mean: 0.2526186406612396\n",
      "Positive similarity mean: 0.8624925017356873\n",
      "Negative similarity mean: 0.3767496645450592\n",
      "Positive loss mean: 0.0730508491396904\n",
      "Negative loss mean: 0.19100989401340485\n",
      "Positive similarity mean: 0.7868560552597046\n",
      "Negative similarity mean: 0.4568706154823303\n",
      "Positive loss mean: 0.11489789187908173\n",
      "Negative loss mean: 0.2231450229883194\n",
      "Positive similarity mean: 0.8712812066078186\n",
      "Negative similarity mean: 0.4410375952720642\n",
      "Positive loss mean: 0.05832571163773537\n",
      "Negative loss mean: 0.2563484311103821\n",
      "Positive similarity mean: 0.8395699858665466\n",
      "Negative similarity mean: 0.39669162034988403\n",
      "Positive loss mean: 0.08836179226636887\n",
      "Negative loss mean: 0.19327200949192047\n",
      "Positive similarity mean: 0.8255773782730103\n",
      "Negative similarity mean: 0.4971272349357605\n",
      "Positive loss mean: 0.09811271727085114\n",
      "Negative loss mean: 0.22656823694705963\n",
      "Positive similarity mean: 0.7801920771598816\n",
      "Negative similarity mean: 0.46921804547309875\n",
      "Positive loss mean: 0.12278333306312561\n",
      "Negative loss mean: 0.22199256718158722\n",
      "Positive similarity mean: 0.8497297167778015\n",
      "Negative similarity mean: 0.44513407349586487\n",
      "Positive loss mean: 0.07337415218353271\n",
      "Negative loss mean: 0.2408003807067871\n",
      "Positive similarity mean: 0.782652735710144\n",
      "Negative similarity mean: 0.39585453271865845\n",
      "Positive loss mean: 0.11037164181470871\n",
      "Negative loss mean: 0.21236681938171387\n",
      "Positive similarity mean: 0.8267615437507629\n",
      "Negative similarity mean: 0.442793607711792\n",
      "Positive loss mean: 0.08188226819038391\n",
      "Negative loss mean: 0.24836382269859314\n",
      "Positive similarity mean: 0.8008565306663513\n",
      "Negative similarity mean: 0.43336164951324463\n",
      "Positive loss mean: 0.10812870413064957\n",
      "Negative loss mean: 0.217649444937706\n",
      "Positive similarity mean: 0.8101444840431213\n",
      "Negative similarity mean: 0.47890567779541016\n",
      "Positive loss mean: 0.09789426624774933\n",
      "Negative loss mean: 0.24298444390296936\n",
      "Positive similarity mean: 0.8131554126739502\n",
      "Negative similarity mean: 0.4293884336948395\n",
      "Positive loss mean: 0.08612369000911713\n",
      "Negative loss mean: 0.24631616473197937\n",
      "Positive similarity mean: 0.8628888130187988\n",
      "Negative similarity mean: 0.4444088339805603\n",
      "Positive loss mean: 0.07069794833660126\n",
      "Negative loss mean: 0.2287123203277588\n",
      "Positive similarity mean: 0.8268831372261047\n",
      "Negative similarity mean: 0.4953988790512085\n",
      "Positive loss mean: 0.0946732684969902\n",
      "Negative loss mean: 0.23744305968284607\n",
      "Positive similarity mean: 0.8112918734550476\n",
      "Negative similarity mean: 0.4730698764324188\n",
      "Positive loss mean: 0.10172541439533234\n",
      "Negative loss mean: 0.2322005331516266\n",
      "Positive similarity mean: 0.8241543769836426\n",
      "Negative similarity mean: 0.44346585869789124\n",
      "Positive loss mean: 0.08998347818851471\n",
      "Negative loss mean: 0.2312149554491043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330/330 [00:00<00:00, 425.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive similarity mean: 0.846086323261261\n",
      "Negative similarity mean: 0.4847847819328308\n",
      "Positive loss mean: 0.08597521483898163\n",
      "Negative loss mean: 0.22829216718673706\n",
      "Positive similarity mean: 0.8399059176445007\n",
      "Negative similarity mean: 0.3494550883769989\n",
      "Positive loss mean: 0.08817682415246964\n",
      "Negative loss mean: 0.17498397827148438\n",
      "Positive similarity mean: 0.8489322662353516\n",
      "Negative similarity mean: 0.4351396858692169\n",
      "Positive loss mean: 0.07907451689243317\n",
      "Negative loss mean: 0.22048689424991608\n",
      "Positive similarity mean: 0.8569515347480774\n",
      "Negative similarity mean: 0.46701809763908386\n",
      "Positive loss mean: 0.06873030960559845\n",
      "Negative loss mean: 0.26039010286331177\n",
      "Positive similarity mean: 0.8455838561058044\n",
      "Negative similarity mean: 0.46987050771713257\n",
      "Positive loss mean: 0.07238255441188812\n",
      "Negative loss mean: 0.2673521339893341\n",
      "Positive similarity mean: 0.8107692003250122\n",
      "Negative similarity mean: 0.3752313554286957\n",
      "Positive loss mean: 0.09313701093196869\n",
      "Negative loss mean: 0.2090200036764145\n",
      "Positive similarity mean: 0.8177516460418701\n",
      "Negative similarity mean: 0.4459608793258667\n",
      "Positive loss mean: 0.0946836844086647\n",
      "Negative loss mean: 0.22479891777038574\n",
      "Positive similarity mean: 0.7863691449165344\n",
      "Negative similarity mean: 0.41501396894454956\n",
      "Positive loss mean: 0.10013946145772934\n",
      "Negative loss mean: 0.2361697405576706\n",
      "Positive similarity mean: 0.8471313714981079\n",
      "Negative similarity mean: 0.45648810267448425\n",
      "Positive loss mean: 0.07882288843393326\n",
      "Negative loss mean: 0.23409944772720337\n",
      "Positive similarity mean: 0.8375495076179504\n",
      "Negative similarity mean: 0.465170294046402\n",
      "Positive loss mean: 0.08312895894050598\n",
      "Negative loss mean: 0.24065722525119781\n",
      "Positive similarity mean: 0.788633406162262\n",
      "Negative similarity mean: 0.5303829908370972\n",
      "Positive loss mean: 0.10981155931949615\n",
      "Negative loss mean: 0.2648268938064575\n",
      "Positive similarity mean: 0.8577357530593872\n",
      "Negative similarity mean: 0.452258825302124\n",
      "Positive loss mean: 0.07113215327262878\n",
      "Negative loss mean: 0.23843637108802795\n",
      "Positive similarity mean: 0.8386876583099365\n",
      "Negative similarity mean: 0.4545363783836365\n",
      "Positive loss mean: 0.07876576483249664\n",
      "Negative loss mean: 0.25033214688301086\n",
      "Positive similarity mean: 0.8062502145767212\n",
      "Negative similarity mean: 0.4520869255065918\n",
      "Positive loss mean: 0.09914542734622955\n",
      "Negative loss mean: 0.2357233613729477\n",
      "Positive similarity mean: 0.8798542022705078\n",
      "Negative similarity mean: 0.48945459723472595\n",
      "Positive loss mean: 0.06429678201675415\n",
      "Negative loss mean: 0.2383124828338623\n",
      "Positive similarity mean: 0.81172776222229\n",
      "Negative similarity mean: 0.4417530298233032\n",
      "Positive loss mean: 0.08972351253032684\n",
      "Negative loss mean: 0.24968118965625763\n",
      "Positive similarity mean: 0.8056095242500305\n",
      "Negative similarity mean: 0.3917498290538788\n",
      "Positive loss mean: 0.09187985956668854\n",
      "Negative loss mean: 0.2234124094247818\n",
      "Positive similarity mean: 0.8285459280014038\n",
      "Negative similarity mean: 0.49313753843307495\n",
      "Positive loss mean: 0.08572705090045929\n",
      "Negative loss mean: 0.26157864928245544\n",
      "Positive similarity mean: 0.8147745132446289\n",
      "Negative similarity mean: 0.4452371895313263\n",
      "Positive loss mean: 0.0955069363117218\n",
      "Negative loss mean: 0.23099872469902039\n",
      "Positive similarity mean: 0.8270736336708069\n",
      "Negative similarity mean: 0.46607375144958496\n",
      "Positive loss mean: 0.08578768372535706\n",
      "Negative loss mean: 0.24824239313602448\n",
      "Positive similarity mean: 0.8414940237998962\n",
      "Negative similarity mean: 0.5591413378715515\n",
      "Positive loss mean: 0.07739551365375519\n",
      "Negative loss mean: 0.29993346333503723\n",
      "Positive similarity mean: 0.8596140146255493\n",
      "Negative similarity mean: 0.5332469344139099\n",
      "Positive loss mean: 0.07238653302192688\n",
      "Negative loss mean: 0.2709544003009796\n",
      "Positive similarity mean: 0.8353619575500488\n",
      "Negative similarity mean: 0.4755800664424896\n",
      "Positive loss mean: 0.07524473965167999\n",
      "Negative loss mean: 0.27231326699256897\n",
      "Positive similarity mean: 0.83921217918396\n",
      "Negative similarity mean: 0.39280351996421814\n",
      "Positive loss mean: 0.07536932826042175\n",
      "Negative loss mean: 0.2287568897008896\n",
      "Positive similarity mean: 0.8698782920837402\n",
      "Negative similarity mean: 0.39032936096191406\n",
      "Positive loss mean: 0.06455258280038834\n",
      "Negative loss mean: 0.21657520532608032\n",
      "Positive similarity mean: 0.8450873494148254\n",
      "Negative similarity mean: 0.47995448112487793\n",
      "Positive loss mean: 0.07745632529258728\n",
      "Negative loss mean: 0.25637778639793396\n",
      "Positive similarity mean: 0.7332453727722168\n",
      "Negative similarity mean: 0.534633219242096\n",
      "Positive loss mean: 0.13441932201385498\n",
      "Negative loss mean: 0.2766360342502594\n",
      "Positive similarity mean: 0.7889968156814575\n",
      "Negative similarity mean: 0.40060991048812866\n",
      "Positive loss mean: 0.10715006291866302\n",
      "Negative loss mean: 0.21443229913711548\n",
      "Positive similarity mean: 0.8004981875419617\n",
      "Negative similarity mean: 0.515785276889801\n",
      "Positive loss mean: 0.1059853658080101\n",
      "Negative loss mean: 0.25421619415283203\n",
      "Positive similarity mean: 0.8064236640930176\n",
      "Negative similarity mean: 0.4818114936351776\n",
      "Positive loss mean: 0.10434973984956741\n",
      "Negative loss mean: 0.23583757877349854\n",
      "Positive similarity mean: 0.8496106863021851\n",
      "Negative similarity mean: 0.482483446598053\n",
      "Positive loss mean: 0.07460722327232361\n",
      "Negative loss mean: 0.25845810770988464\n",
      "Positive similarity mean: 0.7901516556739807\n",
      "Negative similarity mean: 0.4702555239200592\n",
      "Positive loss mean: 0.10574394464492798\n",
      "Negative loss mean: 0.24995991587638855\n",
      "Positive similarity mean: 0.8244853019714355\n",
      "Negative similarity mean: 0.3748898208141327\n",
      "Positive loss mean: 0.08912856131792068\n",
      "Negative loss mean: 0.20179422199726105\n",
      "Positive similarity mean: 0.7951521873474121\n",
      "Negative similarity mean: 0.44973400235176086\n",
      "Positive loss mean: 0.09842297434806824\n",
      "Negative loss mean: 0.24903260171413422\n",
      "Positive similarity mean: 0.807502031326294\n",
      "Negative similarity mean: 0.49667447805404663\n",
      "Positive loss mean: 0.08872957527637482\n",
      "Negative loss mean: 0.28244441747665405\n",
      "Positive similarity mean: 0.8244966864585876\n",
      "Negative similarity mean: 0.46345019340515137\n",
      "Positive loss mean: 0.09734950959682465\n",
      "Negative loss mean: 0.21810829639434814\n",
      "Positive similarity mean: 0.8000853061676025\n",
      "Negative similarity mean: 0.4831404387950897\n",
      "Positive loss mean: 0.10620466619729996\n",
      "Negative loss mean: 0.24320237338542938\n",
      "Positive similarity mean: 0.8531241416931152\n",
      "Negative similarity mean: 0.4318770468235016\n",
      "Positive loss mean: 0.06597939133644104\n",
      "Negative loss mean: 0.2508416175842285\n",
      "Positive similarity mean: 0.817433774471283\n",
      "Negative similarity mean: 0.47241929173469543\n",
      "Positive loss mean: 0.0934225395321846\n",
      "Negative loss mean: 0.2464710921049118\n",
      "Positive similarity mean: 0.8440592288970947\n",
      "Negative similarity mean: 0.4579532742500305\n",
      "Positive loss mean: 0.07857952266931534\n",
      "Negative loss mean: 0.242118239402771\n",
      "Positive similarity mean: 0.7780317664146423\n",
      "Negative similarity mean: 0.42071640491485596\n",
      "Positive loss mean: 0.10924996435642242\n",
      "Negative loss mean: 0.23177175223827362\n",
      "Positive similarity mean: 0.8208487629890442\n",
      "Negative similarity mean: 0.5022376179695129\n",
      "Positive loss mean: 0.09167502820491791\n",
      "Negative loss mean: 0.25942814350128174\n",
      "Positive similarity mean: 0.803982138633728\n",
      "Negative similarity mean: 0.4850209355354309\n",
      "Positive loss mean: 0.10490019619464874\n",
      "Negative loss mean: 0.23876558244228363\n",
      "Positive similarity mean: 0.8123140931129456\n",
      "Negative similarity mean: 0.48961105942726135\n",
      "Positive loss mean: 0.08577832579612732\n",
      "Negative loss mean: 0.27820539474487305\n",
      "Positive similarity mean: 0.8689616918563843\n",
      "Negative similarity mean: 0.42076215147972107\n",
      "Positive loss mean: 0.06756659597158432\n",
      "Negative loss mean: 0.2177625596523285\n",
      "Positive similarity mean: 0.8787119388580322\n",
      "Negative similarity mean: 0.45763808488845825\n",
      "Positive loss mean: 0.06396050751209259\n",
      "Negative loss mean: 0.2301272600889206\n",
      "Positive similarity mean: 0.7843209505081177\n",
      "Negative similarity mean: 0.4038679599761963\n",
      "Positive loss mean: 0.11879196763038635\n",
      "Negative loss mean: 0.19723813235759735\n",
      "Positive similarity mean: 0.8443522453308105\n",
      "Negative similarity mean: 0.4325801432132721\n",
      "Positive loss mean: 0.08633587509393692\n",
      "Negative loss mean: 0.2058914452791214\n",
      "Positive similarity mean: 0.8416519165039062\n",
      "Negative similarity mean: 0.3804682195186615\n",
      "Positive loss mean: 0.07669985294342041\n",
      "Negative loss mean: 0.2120758444070816\n",
      "Positive similarity mean: 0.7998890280723572\n",
      "Negative similarity mean: 0.5090647339820862\n",
      "Positive loss mean: 0.10709062218666077\n",
      "Negative loss mean: 0.24924170970916748\n",
      "Positive similarity mean: 0.8542032837867737\n",
      "Negative similarity mean: 0.3919795751571655\n",
      "Positive loss mean: 0.06606412678956985\n",
      "Negative loss mean: 0.23590515553951263\n",
      "Positive similarity mean: 0.8485098481178284\n",
      "Negative similarity mean: 0.42971399426460266\n",
      "Positive loss mean: 0.07278630137443542\n",
      "Negative loss mean: 0.23991245031356812\n",
      "Positive similarity mean: 0.8328968286514282\n",
      "Negative similarity mean: 0.46725210547447205\n",
      "Positive loss mean: 0.08616254478693008\n",
      "Negative loss mean: 0.24087151885032654\n",
      "Positive similarity mean: 0.7823208570480347\n",
      "Negative similarity mean: 0.3792446553707123\n",
      "Positive loss mean: 0.10033653676509857\n",
      "Negative loss mean: 0.22352303564548492\n",
      "Positive similarity mean: 0.8299822211265564\n",
      "Negative similarity mean: 0.5395751595497131\n",
      "Positive loss mean: 0.09032197296619415\n",
      "Negative loss mean: 0.26778751611709595\n",
      "Positive similarity mean: 0.8408745527267456\n",
      "Negative similarity mean: 0.4926278591156006\n",
      "Positive loss mean: 0.08702178299427032\n",
      "Negative loss mean: 0.234648659825325\n",
      "Positive similarity mean: 0.8557577729225159\n",
      "Negative similarity mean: 0.5347914695739746\n",
      "Positive loss mean: 0.06874041259288788\n",
      "Negative loss mean: 0.29357144236564636\n",
      "Positive similarity mean: 0.7975011467933655\n",
      "Negative similarity mean: 0.4286643862724304\n",
      "Positive loss mean: 0.10836854577064514\n",
      "Negative loss mean: 0.2122308313846588\n",
      "Positive similarity mean: 0.8324246406555176\n",
      "Negative similarity mean: 0.46411868929862976\n",
      "Positive loss mean: 0.08706061542034149\n",
      "Negative loss mean: 0.23568111658096313\n",
      "Positive similarity mean: 0.8455146551132202\n",
      "Negative similarity mean: 0.44823142886161804\n",
      "Positive loss mean: 0.07362194359302521\n",
      "Negative loss mean: 0.25090673565864563\n",
      "Positive similarity mean: 0.8261980414390564\n",
      "Negative similarity mean: 0.41922056674957275\n",
      "Positive loss mean: 0.08961664140224457\n",
      "Negative loss mean: 0.22238202393054962\n",
      "Positive similarity mean: 0.8071951270103455\n",
      "Negative similarity mean: 0.4225793480873108\n",
      "Positive loss mean: 0.10468699783086777\n",
      "Negative loss mean: 0.20754103362560272\n",
      "Positive similarity mean: 0.8214620351791382\n",
      "Negative similarity mean: 0.4431365728378296\n",
      "Positive loss mean: 0.08229484409093857\n",
      "Negative loss mean: 0.2572457492351532\n",
      "Positive similarity mean: 0.8277387619018555\n",
      "Negative similarity mean: 0.4375740587711334\n",
      "Positive loss mean: 0.08949506282806396\n",
      "Negative loss mean: 0.22265411913394928\n",
      "Positive similarity mean: 0.8814063668251038\n",
      "Negative similarity mean: 0.3143620789051056\n",
      "Positive loss mean: 0.05976003408432007\n",
      "Negative loss mean: 0.17605596780776978\n",
      "Positive similarity mean: 0.8227928280830383\n",
      "Negative similarity mean: 0.470023512840271\n",
      "Positive loss mean: 0.09206464886665344\n",
      "Negative loss mean: 0.2407580316066742\n",
      "Positive similarity mean: 0.8461920022964478\n",
      "Negative similarity mean: 0.4821337163448334\n",
      "Positive loss mean: 0.0732991099357605\n",
      "Negative loss mean: 0.2662059962749481\n",
      "Positive similarity mean: 0.859057605266571\n",
      "Negative similarity mean: 0.4461623430252075\n",
      "Positive loss mean: 0.06881948560476303\n",
      "Negative loss mean: 0.24350281059741974\n",
      "Test Loss: 0.3293, Accuracy: 0.6798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Test loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "threshold = 0.5  # Adjust the threshold as needed\n",
    "\n",
    "num_test_batches = len(X_test_text_tensor) // batch_size\n",
    "\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation for testing\n",
    "    for i in tqdm(range(num_test_batches), total=num_test_batches):\n",
    "        \n",
    "        # Create batches\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        batch_X_text = X_test_text_tensor[start_idx:end_idx]\n",
    "        batch_X_wine = X_test_wine_tensor[start_idx:end_idx]\n",
    "        batch_y = y_test_tensor[start_idx:end_idx]\n",
    "        \n",
    "        # Forward pass\n",
    "        similarity = model(batch_X_text, batch_X_wine)\n",
    "        loss = criterion(similarity, batch_y)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "\n",
    "        batch_y = batch_y.squeeze(1)  # This will make batch_y shape [256]\n",
    "\n",
    "        # Apply threshold to similarity to get predictions\n",
    "        predicted = (similarity >= threshold).float()\n",
    "        \n",
    "        # Calculate accuracy: correct predictions divided by total predictions\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "\n",
    "# Calculate average loss and accuracy\n",
    "average_test_loss = test_loss / num_test_batches\n",
    "accuracy = correct / total\n",
    "\n",
    "# Print the results\n",
    "print(f'Test Loss: {average_test_loss:.4f}, Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
